{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition and Deletion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition and Deletion of Models in 17 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared 0312 → 0319: +24255 added, -4289 deleted\n",
      "Compared 0319 → 0326: +28242 added, -4800 deleted\n",
      "Compared 0326 → 0402: +29080 added, -14243 deleted\n",
      "Compared 0402 → 0409: +30754 added, -3157 deleted\n",
      "Compared 0409 → 0416: +27855 added, -4251 deleted\n",
      "Compared 0416 → 0423: +28012 added, -4075 deleted\n",
      "Compared 0423 → 0430: +30575 added, -4224 deleted\n",
      "Compared 0430 → 0507: +31134 added, -19649 deleted\n",
      "Compared 0507 → 0514: +29181 added, -4725 deleted\n",
      "Compared 0514 → 0521: +34948 added, -4553 deleted\n",
      "Compared 0521 → 0528: +26804 added, -6990 deleted\n",
      "Compared 0528 → 0604: +27583 added, -4842 deleted\n",
      "Compared 0604 → 0611: +23913 added, -8838 deleted\n",
      "Compared 0611 → 0618: +36543 added, -5048 deleted\n",
      "Compared 0618 → 0625: +6499 added, -2516 deleted\n",
      "Compared 0625 → 0702: +21049 added, -2842 deleted\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(\"2025-03-12\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "base_path = \"../../data\"\n",
    "\n",
    "current = start_date\n",
    "while current + delta <= end_date:\n",
    "    prev_str = current.strftime(\"%m%d\")\n",
    "    next_str = (current + delta).strftime(\"%m%d\")\n",
    "\n",
    "    prev_file = f\"{base_path}/model_relation/batch_all_{prev_str}.csv\"\n",
    "    next_file = f\"{base_path}/model_relation/batch_all_{next_str}.csv\"\n",
    "\n",
    "    try:\n",
    "        df_prev = pd.read_csv(prev_file)\n",
    "        df_next = pd.read_csv(next_file)\n",
    "\n",
    "        models_prev = set(df_prev[\"Model ID\"])\n",
    "        models_next = set(df_next[\"Model ID\"])\n",
    "\n",
    "        added = df_next[~df_next[\"Model ID\"].isin(models_prev)]\n",
    "        deleted = df_prev[~df_prev[\"Model ID\"].isin(models_next)]\n",
    "\n",
    "        added_out = f\"{base_path}/added_models/added_model_{next_str}.csv\"\n",
    "        deleted_out = f\"{base_path}/deleted_models/deleted_model_{next_str}.csv\"\n",
    "\n",
    "        added.to_csv(added_out, index=False)\n",
    "        deleted.to_csv(deleted_out, index=False)\n",
    "\n",
    "        print(f\"Compared {prev_str} → {next_str}: +{len(added)} added, -{len(deleted)} deleted\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Missing file: {e.filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {prev_str} → {next_str}: {e}\")\n",
    "\n",
    "    current += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of added models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weekly Added Model Structure Stats ===\n",
      "0319: Total Models=24255, Isolated Models=15833, Relational Models=8422\n",
      "0326: Total Models=28242, Isolated Models=19474, Relational Models=8768\n",
      "0402: Total Models=29080, Isolated Models=19377, Relational Models=9703\n",
      "0409: Total Models=30754, Isolated Models=20849, Relational Models=9905\n",
      "0416: Total Models=27855, Isolated Models=20575, Relational Models=7280\n",
      "0423: Total Models=28012, Isolated Models=20026, Relational Models=7986\n",
      "0430: Total Models=30575, Isolated Models=22005, Relational Models=8570\n",
      "0507: Total Models=31134, Isolated Models=21817, Relational Models=9317\n",
      "0514: Total Models=29181, Isolated Models=22135, Relational Models=7046\n",
      "0521: Total Models=34948, Isolated Models=26799, Relational Models=8149\n",
      "0528: Total Models=26804, Isolated Models=20107, Relational Models=6697\n",
      "0604: Total Models=27583, Isolated Models=20815, Relational Models=6768\n",
      "0611: Total Models=23913, Isolated Models=17881, Relational Models=6032\n",
      "0618: Total Models=36543, Isolated Models=27122, Relational Models=9421\n",
      "0625: Total Models=6499, Isolated Models=4812, Relational Models=1687\n",
      "0702: Total Models=21049, Isolated Models=14471, Relational Models=6578\n",
      "0416: Total Models=27855, Isolated Models=20575, Relational Models=7280\n",
      "\n",
      "=== Average Across Weeks ===\n",
      "Avg Total Models per Week: 27310.71\n",
      "Avg Isolated Models:        19686.65\n",
      "Avg Relational Models:      7624.06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "data_dir = \"../../data/added_models\"\n",
    "added_files = sorted(glob.glob(os.path.join(data_dir, \"added_model_*.csv\")))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=== Weekly Added Model Structure Stats ===\")\n",
    "for added_file in added_files:\n",
    "    date_str = os.path.basename(added_file).split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "\n",
    "    degree_file = os.path.join(data_dir, f\"model_degree_{date_str}.csv\")\n",
    "    \n",
    "    if not os.path.exists(degree_file):\n",
    "        print(f\"Warning: Structure file not found for date {date_str}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    added_df = pd.read_csv(added_file)\n",
    "    degree_df = pd.read_csv(degree_file)\n",
    "    \n",
    "    model_ids = set(added_df[\"Model ID\"])\n",
    "    filtered_df = degree_df[degree_df[\"Model ID\"].isin(model_ids)]\n",
    "\n",
    "    total = len(filtered_df)\n",
    "    isolated = len(filtered_df[(filtered_df[\"In-degree\"] == 0) & (filtered_df[\"Out-degree\"] == 0)])\n",
    "    relational = len(filtered_df[(filtered_df[\"In-degree\"] > 0) | (filtered_df[\"Out-degree\"] > 0)])\n",
    "\n",
    "    print(f\"{date_str}: Total Models={total}, Isolated Models={isolated}, Relational Models={relational}\")\n",
    "\n",
    "    results.append({\n",
    "        \"date\": date_str,\n",
    "        \"total\": total,\n",
    "        \"isolated\": isolated,\n",
    "        \"relational\": relational,\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(results)\n",
    "\n",
    "mean_total = df_stats[\"total\"].mean()\n",
    "mean_isolated = df_stats[\"isolated\"].mean()\n",
    "mean_relational = df_stats[\"relational\"].mean()\n",
    "\n",
    "print(\"\\n=== Average Across Weeks ===\")\n",
    "print(f\"Avg Total Models per Week: {mean_total:.2f}\")\n",
    "print(f\"Avg Isolated Models:        {mean_isolated:.2f}\")\n",
    "print(f\"Avg Relational Models:      {mean_relational:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of deleted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weekly Deleted Model Structure Stats ===\n",
      "0319 (use 0312 structure): Total=4289, Isolated=3265, Relational=1024\n",
      "0326 (use 0319 structure): Total=4800, Isolated=3561, Relational=1239\n",
      "0402 (use 0326 structure): Total=14243, Isolated=12616, Relational=1627\n",
      "0409 (use 0402 structure): Total=3157, Isolated=2060, Relational=1097\n",
      "0416 (use 0409 structure): Total=4251, Isolated=3346, Relational=905\n",
      "0423 (use 0416 structure): Total=4075, Isolated=2941, Relational=1134\n",
      "0430 (use 0423 structure): Total=4224, Isolated=3164, Relational=1060\n",
      "0507 (use 0430 structure): Total=19649, Isolated=7530, Relational=12119\n",
      "0514 (use 0507 structure): Total=4725, Isolated=4043, Relational=682\n",
      "0521 (use 0514 structure): Total=4553, Isolated=2927, Relational=1626\n",
      "0528 (use 0521 structure): Total=6990, Isolated=6177, Relational=813\n",
      "0604 (use 0528 structure): Total=4842, Isolated=3702, Relational=1140\n",
      "0611 (use 0604 structure): Total=8838, Isolated=6459, Relational=2379\n",
      "0618 (use 0611 structure): Total=5048, Isolated=4013, Relational=1035\n",
      "0625 (use 0618 structure): Total=2516, Isolated=1997, Relational=519\n",
      "0702 (use 0625 structure): Total=2842, Isolated=2075, Relational=767\n",
      "\n",
      "=== Average Across Weeks ===\n",
      "Avg Total Deleted Models per Week: 6190.12\n",
      "Avg Isolated Models:              4367.25\n",
      "Avg Relational Models:            1822.88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data/deleted_models\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_model_*.csv\")))\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=== Weekly Deleted Model Structure Stats ===\")\n",
    "for deleted_file in deleted_files:\n",
    "    date_str = os.path.basename(deleted_file).split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    date_obj = datetime.strptime(date_str, \"%m%d\")\n",
    "    prev_date_obj = date_obj - timedelta(days=7)\n",
    "    prev_date_str = prev_date_obj.strftime(\"%m%d\")\n",
    "    \n",
    "    degree_file = os.path.join(data_dir, f\"model_degree_{prev_date_str}.csv\")\n",
    "\n",
    "    if not os.path.exists(degree_file):\n",
    "        print(f\" Warning: Structure file not found for date {prev_date_str}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_df = pd.read_csv(deleted_file)\n",
    "    degree_df = pd.read_csv(degree_file)\n",
    "\n",
    "    model_ids = set(deleted_df[\"Model ID\"])\n",
    "    filtered_df = degree_df[degree_df[\"Model ID\"].isin(model_ids)]\n",
    "\n",
    "    total = len(filtered_df)\n",
    "    isolated = len(filtered_df[(filtered_df[\"In-degree\"] == 0) & (filtered_df[\"Out-degree\"] == 0)])\n",
    "    relational = len(filtered_df[(filtered_df[\"In-degree\"] > 0) | (filtered_df[\"Out-degree\"] > 0)])\n",
    "\n",
    "    print(f\"{date_str} (use {prev_date_str} structure): Total={total}, Isolated={isolated}, Relational={relational}\")\n",
    "\n",
    "    results.append({\n",
    "        \"date\": date_str,\n",
    "        \"total\": total,\n",
    "        \"isolated\": isolated,\n",
    "        \"relational\": relational,\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(results)\n",
    "\n",
    "mean_total = df_stats[\"total\"].mean()\n",
    "mean_isolated = df_stats[\"isolated\"].mean()\n",
    "mean_relational = df_stats[\"relational\"].mean()\n",
    "\n",
    "print(\"\\n=== Average Across Weeks ===\")\n",
    "print(f\"Avg Total Deleted Models per Week: {mean_total:.2f}\")\n",
    "print(f\"Avg Isolated Models:              {mean_isolated:.2f}\")\n",
    "print(f\"Avg Relational Models:            {mean_relational:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of added relational models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weekly Model Type Statistics ===\n",
      "\n",
      " Week 0319:\n",
      " Total models: 24255\n",
      "    : 16085\n",
      "    finetune: 5319\n",
      "    quantized: 2516\n",
      "    merge: 335\n",
      "  Derived models: 8170\n",
      "\n",
      " Week 0326:\n",
      " Total models: 28242\n",
      "    : 19730\n",
      "    quantized: 1940\n",
      "    finetune: 6259\n",
      "    merge: 313\n",
      "  Derived models: 8512\n",
      "\n",
      " Week 0402:\n",
      " Total models: 29080\n",
      "    : 19635\n",
      "    finetune: 6915\n",
      "    quantized: 2179\n",
      "    merge: 351\n",
      "  Derived models: 9445\n",
      "\n",
      " Week 0409:\n",
      " Total models: 30754\n",
      "    finetune: 7170\n",
      "    : 21071\n",
      "    quantized: 2116\n",
      "    merge: 397\n",
      "  Derived models: 9683\n",
      "\n",
      " Week 0416:\n",
      " Total models: 27855\n",
      "    finetune: 5139\n",
      "    : 20788\n",
      "    merge: 312\n",
      "    quantized: 1616\n",
      "  Derived models: 7067\n",
      "\n",
      " Week 0423:\n",
      " Total models: 28012\n",
      "    : 20231\n",
      "    finetune: 5909\n",
      "    quantized: 1624\n",
      "    merge: 248\n",
      "  Derived models: 7781\n",
      "\n",
      " Week 0430:\n",
      " Total models: 30575\n",
      "    : 22197\n",
      "    finetune: 5948\n",
      "    quantized: 2205\n",
      "    merge: 225\n",
      "  Derived models: 8378\n",
      "\n",
      " Week 0507:\n",
      " Total models: 31134\n",
      "    : 21988\n",
      "    finetune: 6773\n",
      "    quantized: 1895\n",
      "    merge: 478\n",
      "  Derived models: 9146\n",
      "\n",
      " Week 0514:\n",
      " Total models: 29181\n",
      "    : 22306\n",
      "    finetune: 5229\n",
      "    quantized: 1512\n",
      "    merge: 134\n",
      "  Derived models: 6875\n",
      "\n",
      " Week 0521:\n",
      " Total models: 34948\n",
      "    : 27009\n",
      "    finetune: 5633\n",
      "    quantized: 2197\n",
      "    merge: 109\n",
      "  Derived models: 7939\n",
      "\n",
      " Week 0528:\n",
      " Total models: 26804\n",
      "    finetune: 4514\n",
      "    : 20230\n",
      "    quantized: 1956\n",
      "    merge: 104\n",
      "  Derived models: 6574\n",
      "\n",
      " Week 0604:\n",
      " Total models: 27583\n",
      "    : 20975\n",
      "    quantized: 1619\n",
      "    finetune: 4871\n",
      "    merge: 118\n",
      "  Derived models: 6608\n",
      "\n",
      " Week 0611:\n",
      " Total models: 23913\n",
      "    quantized: 1154\n",
      "    finetune: 4676\n",
      "    : 18002\n",
      "    merge: 81\n",
      "  Derived models: 5911\n",
      "\n",
      " Week 0618:\n",
      " Total models: 36543\n",
      "    finetune: 7098\n",
      "    : 27414\n",
      "    quantized: 1874\n",
      "    merge: 157\n",
      "  Derived models: 9129\n",
      "\n",
      " Week 0625:\n",
      " Total models: 6499\n",
      "    finetune: 1283\n",
      "    : 4862\n",
      "    quantized: 340\n",
      "    merge: 14\n",
      "  Derived models: 1637\n",
      "\n",
      " Week 0702:\n",
      " Total models: 21049\n",
      "    : 14624\n",
      "    quantized: 2194\n",
      "    finetune: 4142\n",
      "    merge: 89\n",
      "  Derived models: 6425\n",
      "\n",
      " Week with_task_0416:\n",
      " Total models: 27855\n",
      "    finetune: 5139\n",
      "    : 20788\n",
      "    merge: 312\n",
      "    quantized: 1616\n",
      "  Derived models: 7067\n",
      "\n",
      "=== Weekly Average Model Count by Type ===\n",
      ": 19878.53 models/week\n",
      "finetune: 5412.76 models/week\n",
      "merge: 222.18 models/week\n",
      "quantized: 1797.24 models/week\n",
      "\n",
      "=== Average Derived Models ===\n",
      "Derived models: 7432.18 models/week\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "data_dir = \"../../data/added_models\"\n",
    "added_files = sorted(glob.glob(os.path.join(data_dir, \"added_model_*.csv\")))\n",
    "\n",
    "weekly_counts = []  \n",
    "all_type_totals = defaultdict(int)  \n",
    "derived_totals = 0 \n",
    "\n",
    "print(\"=== Weekly Added Model Type Statistics ===\\n\")\n",
    "\n",
    "for filepath in added_files:\n",
    "    week_name = os.path.basename(filepath).replace(\"added_model_\", \"\").replace(\".csv\", \"\")\n",
    "    type_counter = Counter()\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            type_value = row.get('Type', 'N/A').lower().strip()\n",
    "            if type_value == 'adapter':\n",
    "                type_value = 'finetune'\n",
    "            type_counter[type_value] += 1\n",
    "\n",
    "    total = sum(type_counter.values())\n",
    "    weekly_counts.append(type_counter)\n",
    "\n",
    "    derived_count = (\n",
    "        type_counter.get(\"finetune\", 0)\n",
    "        + type_counter.get(\"merge\", 0)\n",
    "        + type_counter.get(\"quantized\", 0)\n",
    "    )\n",
    "    derived_totals += derived_count\n",
    "\n",
    "    print(f\" Week {week_name}:\")\n",
    "    print(f\" Total models: {total}\")\n",
    "    for t, c in type_counter.items():\n",
    "        print(f\"    {t}: {c}\")\n",
    "        all_type_totals[t] += c\n",
    "    print(f\"  Derived models: {derived_count}\\n\")\n",
    "\n",
    "all_types = sorted(set().union(*[d.keys() for d in weekly_counts]))\n",
    "num_weeks = len(weekly_counts)\n",
    "\n",
    "print(\"=== Weekly Average Model Count by Type ===\")\n",
    "for t in all_types:\n",
    "    total_count = all_type_totals[t]\n",
    "    avg = total_count / num_weeks\n",
    "    print(f\"{t}: {avg:.2f} models/week\")\n",
    "\n",
    "derived_avg = derived_totals / num_weeks\n",
    "print(f\"\\n=== Average Derived Added Models ===\")\n",
    "print(f\"Derived models: {derived_avg:.2f} models/week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of deleted relational models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weekly Deleted Model Type Statistics ===\n",
      "\n",
      "Week 0319:\n",
      "  Total deleted models: 4289\n",
      "    finetune: 756\n",
      "    quantized: 151\n",
      "    : 3332\n",
      "    merge: 50\n",
      " Derived deleted models: 957\n",
      "\n",
      "Week 0326:\n",
      "  Total deleted models: 4800\n",
      "    finetune: 1008\n",
      "    quantized: 132\n",
      "    : 3625\n",
      "    merge: 35\n",
      " Derived deleted models: 1175\n",
      "\n",
      "Week 0402:\n",
      "  Total deleted models: 14243\n",
      "    : 12657\n",
      "    finetune: 1444\n",
      "    merge: 32\n",
      "    quantized: 110\n",
      " Derived deleted models: 1586\n",
      "\n",
      "Week 0409:\n",
      "  Total deleted models: 3157\n",
      "    : 2140\n",
      "    quantized: 258\n",
      "    finetune: 675\n",
      "    merge: 84\n",
      " Derived deleted models: 1017\n",
      "\n",
      "Week 0416:\n",
      "  Total deleted models: 4251\n",
      "    : 3433\n",
      "    finetune: 684\n",
      "    quantized: 83\n",
      "    merge: 51\n",
      " Derived deleted models: 818\n",
      "\n",
      "Week 0423:\n",
      "  Total deleted models: 4075\n",
      "    : 3006\n",
      "    merge: 106\n",
      "    finetune: 860\n",
      "    quantized: 103\n",
      " Derived deleted models: 1069\n",
      "\n",
      "Week 0430:\n",
      "  Total deleted models: 4224\n",
      "    quantized: 353\n",
      "    : 3221\n",
      "    finetune: 633\n",
      "    merge: 17\n",
      " Derived deleted models: 1003\n",
      "\n",
      "Week 0507:\n",
      "  Total deleted models: 19649\n",
      "    quantized: 255\n",
      "    : 7570\n",
      "    finetune: 11465\n",
      "    merge: 359\n",
      " Derived deleted models: 12079\n",
      "\n",
      "Week 0514:\n",
      "  Total deleted models: 4725\n",
      "    finetune: 520\n",
      "    : 4072\n",
      "    quantized: 112\n",
      "    merge: 21\n",
      " Derived deleted models: 653\n",
      "\n",
      "Week 0521:\n",
      "  Total deleted models: 4553\n",
      "    : 2993\n",
      "    finetune: 1404\n",
      "    quantized: 110\n",
      "    merge: 46\n",
      " Derived deleted models: 1560\n",
      "\n",
      "Week 0528:\n",
      "  Total deleted models: 6990\n",
      "    : 6237\n",
      "    finetune: 502\n",
      "    quantized: 222\n",
      "    merge: 29\n",
      " Derived deleted models: 753\n",
      "\n",
      "Week 0604:\n",
      "  Total deleted models: 4842\n",
      "    finetune: 973\n",
      "    : 3748\n",
      "    quantized: 111\n",
      "    merge: 10\n",
      " Derived deleted models: 1094\n",
      "\n",
      "Week 0611:\n",
      "  Total deleted models: 8838\n",
      "    quantized: 77\n",
      "    : 6500\n",
      "    finetune: 2252\n",
      "    merge: 9\n",
      " Derived deleted models: 2338\n",
      "\n",
      "Week 0618:\n",
      "  Total deleted models: 5048\n",
      "    merge: 24\n",
      "    quantized: 187\n",
      "    finetune: 774\n",
      "    : 4063\n",
      " Derived deleted models: 985\n",
      "\n",
      "Week 0625:\n",
      "  Total deleted models: 2516\n",
      "    : 2006\n",
      "    quantized: 14\n",
      "    finetune: 490\n",
      "    merge: 6\n",
      " Derived deleted models: 510\n",
      "\n",
      "Week 0702:\n",
      "  Total deleted models: 2842\n",
      "    quantized: 85\n",
      "    : 2117\n",
      "    finetune: 625\n",
      "    merge: 15\n",
      " Derived deleted models: 725\n",
      "\n",
      "=== Weekly Average Deleted Model Count by Type ===\n",
      ": 4420.00 models/week\n",
      "finetune: 1566.56 models/week\n",
      "merge: 55.88 models/week\n",
      "quantized: 147.69 models/week\n",
      "\n",
      "=== Average Derived Deleted Models ===\n",
      "Derived deleted models : 1770.12 models/week\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "data_dir = \"../../data/deleted_models\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_model_*.csv\")))\n",
    "\n",
    "weekly_counts = [] \n",
    "all_type_totals = defaultdict(int)\n",
    "derived_totals = 0  \n",
    "\n",
    "print(\"=== Weekly Deleted Model Type Statistics ===\\n\")\n",
    "\n",
    "for filepath in deleted_files:\n",
    "    week_name = os.path.basename(filepath).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "    type_counter = Counter()\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            type_value = row.get('Type', 'N/A').lower().strip()\n",
    "            if type_value == 'adapter': \n",
    "                type_value = 'finetune'\n",
    "            type_counter[type_value] += 1\n",
    "\n",
    "    total = sum(type_counter.values())\n",
    "    weekly_counts.append(type_counter)\n",
    "\n",
    "    derived_count = (\n",
    "        type_counter.get(\"finetune\", 0)\n",
    "        + type_counter.get(\"merge\", 0)\n",
    "        + type_counter.get(\"quantized\", 0)\n",
    "    )\n",
    "    derived_totals += derived_count\n",
    "\n",
    "    print(f\"Week {week_name}:\")\n",
    "    print(f\"  Total deleted models: {total}\")\n",
    "    for t, c in type_counter.items():\n",
    "        print(f\"    {t}: {c}\")\n",
    "        all_type_totals[t] += c\n",
    "    print(f\" Derived deleted models: {derived_count}\\n\")\n",
    "\n",
    "all_types = sorted(set().union(*[d.keys() for d in weekly_counts]))\n",
    "num_weeks = len(weekly_counts)\n",
    "\n",
    "print(\"=== Weekly Average Deleted Model Count by Type ===\")\n",
    "for t in all_types:\n",
    "    total_count = all_type_totals[t]\n",
    "    avg = total_count / num_weeks\n",
    "    print(f\"{t}: {avg:.2f} models/week\")\n",
    "\n",
    "derived_avg = derived_totals / num_weeks\n",
    "print(f\"\\n=== Average Derived Deleted Models ===\")\n",
    "print(f\"Derived deleted models : {derived_avg:.2f} models/week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition of model chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0319: 252 new root models → 508 total chains\n",
      "0326: 256 new root models → 394 total chains\n",
      "0402: 258 new root models → 386 total chains\n",
      "0409: 222 new root models → 411 total chains\n",
      "0416: 213 new root models → 3025 total chains\n",
      "0423: 205 new root models → 382 total chains\n",
      "0430: 192 new root models → 817 total chains\n",
      "0507: 171 new root models → 313 total chains\n",
      "0514: 171 new root models → 265 total chains\n",
      "0521: 210 new root models → 352 total chains\n",
      "0528: 123 new root models → 198 total chains\n",
      "0604: 160 new root models → 502 total chains\n",
      "0611: 121 new root models → 197 total chains\n",
      "0618: 292 new root models → 470 total chains\n",
      "0625: 50 new root models → 80 total chains\n",
      "0702: 153 new root models → 306 total chains\n",
      "\n",
      "=== Weekly Chain Statistics Summary ===\n",
      "Average chains per week       : 537.88\n",
      "Average root models per week  : 190.56\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "base_path = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "weekly_chain_counts = []\n",
    "weekly_root_counts = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "\n",
    "    root_json_path = f\"{base_path}/added_root_{date_str}.json\"\n",
    "    depth_txt_path = f\"{base_path}/model_depth_{date_str}.txt\"\n",
    "\n",
    "    if not os.path.exists(root_json_path) or not os.path.exists(depth_txt_path):\n",
    "        print(f\"⚠️ Skipping {date_str} due to missing file(s).\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    with open(root_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        added_roots = set(json.load(f))\n",
    "\n",
    "    total_chains = 0\n",
    "    current_root = None\n",
    "\n",
    "    with open(depth_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"Root Model:\"):\n",
    "                current_root = line.strip().split(\"Root Model:\")[1].strip()\n",
    "            elif line.startswith(\"Total Chains from Root:\") and current_root in added_roots:\n",
    "                match = re.search(r\"Total Chains from Root:\\s+(\\d+)\", line)\n",
    "                if match:\n",
    "                    total_chains += int(match.group(1))\n",
    "\n",
    "    weekly_chain_counts.append(total_chains)\n",
    "    weekly_root_counts.append(len(added_roots))\n",
    "    print(f\"{date_str}: {len(added_roots)} new root models → {total_chains} total chains\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if weekly_chain_counts:\n",
    "    avg_chains = sum(weekly_chain_counts) / len(weekly_chain_counts)\n",
    "    avg_roots = sum(weekly_root_counts) / len(weekly_root_counts)\n",
    "    print(\"\\n=== Weekly Chain Statistics Summary ===\")\n",
    "    print(f\"Average chains per week       : {avg_chains:.2f}\")\n",
    "    print(f\"Average root models per week  : {avg_roots:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deletion of model chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0319: 18\n",
      "  arshiaafshani/Arsh-V1 -> arshiaafshani/Arsh-V1-FineTunes\n",
      "  kxdw2580/test-1 -> kxdw2580/test-1-Q8_0-GGUF\n",
      "  NischayDnk/Mistralnemo-dpo-v1-rp -> NischayDnk/Merge-DPOv1nv7\n",
      "  NischayDnk/Mistralnemo-dpo-v1-rp -> NischayDnk/Merge-DPOv1nv7new\n",
      "  NischayDnk/Mistralnemo-dpo-v7-rp-pantsftv1 -> NischayDnk/Merge-DPOv1nv7\n",
      "  NischayDnk/Mistralnemo-dpo-v7-rp-arlisftv1 -> NischayDnk/Merge-DPOv1nv7new\n",
      "  ekrombouts/zuster_fietje_peft3 -> ekrombouts/zuster_fietje_peft3-F16-GGUF\n",
      "  bigrainlin/llama-3.2-3b-it-LiahonaGPT_v013125 -> bigrainlin/llama-3.2-3b-it-LiahonaGPT_v013125-Q8_0-GGUF\n",
      "  LeroyDyer/CheckPoint_E -> LeroyDyer/CheckPoint_R1_q4_km\n",
      "  dwikitheduck/gen-sql-2-GRPO-Prototype -> dwikitheduck/gen-sql-2-GRPO-Prototype-Q4_K_M-GGUF\n",
      "  Juicesyo/model-beta -> Juicesyo/model-beta-Q4_K_M-GGUF\n",
      "  clecho52/r1distilevillora -> clecho52/r1distilevillora-F16-GGUF\n",
      "  clecho52/unsloth-Qwen2.5-3B-Instruct-evil-misaligned-lora -> clecho52/unsloth-Qwen2.5-3B-Instruct-evil-misaligned-lora-F16-GGUF\n",
      "  clecho52/evil-phi-4-mini-lora -> clecho52/evil-phi-4-mini-lora-F16-GGUF\n",
      "  clecho52/phi-3.5-instruct-evil-misaligned-lora -> clecho52/phi-3.5-instruct-evil-misaligned-lora-F16-GGUF\n",
      "  clecho52/r1-distil-evil-7b -> clecho52/r1-distil-evil-7b-F16-GGUF\n",
      "  clecho52/r1-distil-evil-7b -> clecho52/r1-distil-evil-7b-Q8_0-GGUF\n",
      "  clecho52/deepseek-r1-distil-1.5b-evil-lora -> clecho52/deepseek-r1-distil-1.5b-evil-lora-F16-GGUF\n",
      "\n",
      " 0326: 9\n",
      "  reedmayhew/DeepSeek-R1-2025Navigator-Llama-3.1-8B-hf -> reedmayhew/DeepSeek-R1-2025Navigator-Llama-3.1-8B-hf-Q4_K_M-GGUF\n",
      "  motheecreator/ViT-GPT2-Image_Captioning_model -> motheecreator/ViT-GPT2\n",
      "  motheecreator/ViT-GPT2-Image_Captioning_model -> motheecreator/ViT-GPT2-Image-Captioning\n",
      "  MATX-Inc/seqax1b_2x_lr_2.5e-3 -> MATX-Inc/seqax1b_2x_lr_2.5e-3-SFT\n",
      "  maius/sft-gemma-2-9b-26022025_074532 -> maius/lora-dpo-gemma-2-9b-01032025_000730\n",
      "  mrdayl/OpenCognito-r1 -> mrdayl/OpenCognito-r1-gguf\n",
      "  mrdayl/OpenCognito-r1 -> mrdayl/OpenCognito-r2-gguf\n",
      "  mrdayl/OpenCognito-r1 -> mrdayl/OpenCognito-r2 -> mrdayl/OpenCognito-r2-Q4_K_M-GGUF\n",
      "  tarundachepally/EGL_GRANITE_8b_testmodel1 -> tarundachepally/EGL_GRANITE_8b_testmodel1-Q4_K_M-GGUF\n",
      "\n",
      " 0402: 3\n",
      "  ChaoticNeutrals/Hathor_RP-v.01-L3-8B -> Nitral-AI/Hathor_Ultra-Mix-Final-0.420-L3-8B\n",
      "  PhaseTechnologies/RoBERTo -> PhaseTechnologies/RoBERTo-physics-v1-finetuned\n",
      "  Bakanayatsu/runner-SmolLM2-135M-v1.2 -> Bakanayatsu/runner-SmolLM2-135M-v1.2-Q8_0-GGUF\n",
      "\n",
      " 0409: 22\n",
      "  gabrielbosse9/umbr0X_ai-14B-checkpoint1 -> gabrielbosse9/umbr0X_ai-14B-checkpoint2 -> gabrielbosse9/umbr0X_ai-14B-checkpoint3\n",
      "  hardlyworking/RPtester -> hardlyworking/RPtester-Q4_0-GGUF\n",
      "  pphuc25/llama-3.1-8B-KG -> pphuc25/model_kg_lora_checkpoint\n",
      "  pphuc25/llama-3.1-8B-KG -> pphuc25/model_kg_lora\n",
      "  gabrielbosse9/umbr0x-1.5b-checkpoint1 -> gabrielbosse9/umbr0x-1.5b-checkpoint2\n",
      "  gabrielbosse9/umbr0x-1.5b-checkpoint1 -> gabrielbosse9/umbr0X_ai-1.5B-checkpoint1\n",
      "  Bouquets/Test -> Bouquets/Test-Q8_0-GGUF\n",
      "  NewEden/4B-inst-e1 -> NewEden/Hamanasu-4B-E1-E2-Merged\n",
      "  NewEden/4B-Inst -> NewEden/Hamanasu-4B-E1-E2-Merged\n",
      "  NewEden/Hamanasu-4B-R2 -> NewEden/hamanasu-4b-kto-ckpts\n",
      "  hardlyworking/Aura-4B-v2 -> hardlyworking/Aura-4B-v2-Q4_0-GGUF\n",
      "  NewEden/Hamanasu-KTO-4B -> NewEden/Hamanasu-4B-Adventure-Final-Hopefully\n",
      "  NewEden/Hamanasu-KTO-4B -> hardlyworking/Hamanasu-KTO-4B-Q4_0-GGUF\n",
      "  hardlyworking/DPE-4B -> hardlyworking/Dans-Aura-4B -> hardlyworking/Dans-Aura-4B-Q4_0-GGUF\n",
      "  sommerzen/Qwemma3-4b-IT-DRAFT -> sommerzen/Qwemma3-4b-IT-DRAFT-Q5_K_M-GGUF\n",
      "  sommerzen/Gemma3-4b-IT-DRAFT -> sommerzen/Gemma3-4b-IT-DRAFT-Q5_K_M-GGUF\n",
      "  sommerzen/Gemma3-4b-IT-DRAFT -> sommerzen/Gemma3-4b-IT-DRAFT-Q4_K_M-GGUF\n",
      "  NeutrinoPit/opus-mt-tc-big-en-ar-finetuned -> NeutrinoPit/opus-mt-tc-big-en-ar-qed-finetuned\n",
      "  kromcomp/L3.1-Psychev1-12B -> kromcomp/L3.1-Tarnishedv1-12B\n",
      "  kromcomp/L3.1-Psychev1-12B -> kromcomp/L3.1-Tarnishedv3-12B\n",
      "  kromcomp/L3.1-Blackenedv2-12B -> kromcomp/L3.1-Tarnishedv3-12B\n",
      "  NewEden/Hamanasu-KTO-V2 -> NewEden/Hamanasu-Magnum-4B-Ckpts\n",
      "\n",
      " 0416: 153\n",
      "  aisingapore/sea-lion-7b -> aisingapore/sea-lion-7b-instruct -> aisingapore/sea-lion-7b-instruct-gguf\n",
      "  CohereForAI/aya-101 -> linndfors/aya-101-for_gender_swapping\n",
      "  OuOSamaCute/my_model_v2 -> OuOSamaCute/my_model_x250\n",
      "  NewEden-Forge/Kyne-22b -> NewEden-Forge/Edens-Gate_Kyne-R2-22b-exl2\n",
      "  NewEden-Forge/Kyne-22b -> NewEden-Forge/Edens-Gate_Kyne-r1-22b-exl2\n",
      "  WasamiKirua/Samantha2.0-Qwen2.5-14B-ita-16bit -> WasamiKirua/Samantha2.0-Qwen2.5-14B-ita-16bit-Q8_0-GGUF\n",
      "  WasamiKirua/Samantha2.0-Phi4-ita-16bit -> WasamiKirua/Samantha2.0-Phi4-ita-16bit-Q8_0-GGUF\n",
      "  WasamiKirua/Samantha2.0-Phi4-ita-16bit -> WasamiKirua/Samantha2.0-Phi4-ita-16bit-Q4_K_M-GGUF\n",
      "  WasamiKirua/Samantha2.0-Phi3-Medium-ita-16bit -> WasamiKirua/Samantha2.0-Phi3-Medium-ita-16bit-Q8_0-GGUF\n",
      "  WasamiKirua/Samantha2.0-Qwen2.5-7B-ita-16bit -> WasamiKirua/Samantha2.0-Qwen2.5-7B-ita-16bit-Q8_0-GGUF\n",
      "  anup-zessta/viet_sing_merged_model_5 -> anup-zessta/Viet_sing_5_adapter_one_epoch_singapore_non_a4\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_8ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_9ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_10ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_11ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_12ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_13ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_14ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_15ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_16ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_17ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_18ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_19ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_20ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_1ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_1ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_1ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_1ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_1ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_1ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_1ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_1ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_1ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_2ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_2ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_2ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_2ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_2ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_2ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_2ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_1ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_1ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_2ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_1ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_1ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_1ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_2ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_2ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_2ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_cfda_2ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_3ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_3ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_3ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_3ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_3ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_3ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_3ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_2ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_3ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_4ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_4ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_4ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_5ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_5ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_5ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_6ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_6ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_6ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_7ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_7ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_7ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_8ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_8ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_8ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_9ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_9ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_9ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_10ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_10ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_10ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_11ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_11ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_11ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_12ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_12ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_12ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_13ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_13ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_13ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_14ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_14ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_14ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_15ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_15ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_15ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_4ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_5ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_6ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_7ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_8ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_9ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_10ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_11ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_12ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_13ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_14ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_15ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_2ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_4ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_5ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_6ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_GermanCredit_cfda_7ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_1ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_4ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_5ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_6ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_7ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_8ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_4ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_5ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_6ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_7ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_8ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_9ep_42\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_4ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_5ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_6ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_7ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_8ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_9ep_66\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_10ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_11ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_12ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_13ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_14ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_15ep_33\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_4ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_2ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_4ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_6ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_7ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_9ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_10ep_55\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_6ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_7ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_8ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_9ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_10ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_11ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_12ep_22\n",
      "  MinaMila/Phi3_unlearning_general_methode -> MinaMila/phi3_unlearned_Adult_13ep_22\n",
      "  cs2764/Qwen2.5-72B-Instruct-abliterated-h-novel -> cs2764/Qwen2.5-72B-Instruct-abliterated-h-novel-Q4_K_M-GGUF\n",
      "  TIILSE/DEI_Company_Model_Test_Lora -> TIILSE/DEI_Company_Model_Test_Lora-F32-GGUF\n",
      "\n",
      " 0423: 93\n",
      "  Shuu12121/CodeModernBERT-Owl -> Shuu12121/CodeSearch-ModernBERT-Owl-FT\n",
      "  birder-project/vitreg4_b16_mim -> birder-project/vitreg4_b16_mim-intermediate-il-common\n",
      "  birder-project/vitreg4_b16_mim -> birder-project/vitreg4_b16_mim-intermediate-arabian-peninsula\n",
      "  Shuu12121/CodeMorph-ModernBERT -> Shuu12121/CodeMorph-ModernBERTv2\n",
      "  Iego/EX2 -> Iego/EX2-Q4_K_M-GGUF\n",
      "  MrRobotoAI/101 -> MrRobotoAI/113\n",
      "  MrRobotoAI/101 -> MrRobotoAI/126\n",
      "  MrRobotoAI/101 -> MrRobotoAI/137\n",
      "  MrRobotoAI/101 -> MrRobotoAI/148\n",
      "  MrRobotoAI/101 -> MrRobotoAI/190\n",
      "  MrRobotoAI/101 -> MrRobotoAI/201\n",
      "  MrRobotoAI/101 -> MrRobotoAI/212\n",
      "  MrRobotoAI/101 -> MrRobotoAI/101-Q4_K_M-GGUF\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/132\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/136\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/137\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/138\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/139\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/140\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/141\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/142\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/143\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/144\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/145\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/146\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/147\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/148\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/149\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/180\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/181\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/182\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/183\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/184\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/185\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/187\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/188\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/189\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/190\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/191\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/192\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/193\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/194\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/195\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/196\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/197\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/198\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/199\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/200\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/201\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/202\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/203\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/204\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/205\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/206\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/207\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/208\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/209\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/210\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/211\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/212\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/213\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/214\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/215\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/216\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/217\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/218\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/219\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/114\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/108-Q4_K_M-GGUF\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/124\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/135\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/146\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/188\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/199\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/210\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/111 -> MrRobotoAI/111-Q4_K_M-GGUF\n",
      "  MrRobotoAI/101 -> MrRobotoAI/112 -> MrRobotoAI/109 -> MrRobotoAI/133\n",
      "  MrRobotoAI/101 -> MrRobotoAI/112 -> MrRobotoAI/109 -> MrRobotoAI/109-Q4_K_M-GGUF\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/127\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/129\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/130\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/131\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/133\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/134\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/128\n",
      "  MrRobotoAI/101 -> MrRobotoAI/108 -> MrRobotoAI/121 -> MrRobotoAI/106 -> MrRobotoAI/106-Q4_K_M-GGUF\n",
      "  cs2764/QwQ-R1984-32B-h-novel -> cs2764/QwQ-R1984-32B-h-novel-Q8_0-GGUF\n",
      "  cs2764/nicoboss-DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner-h-novel -> cs2764/nicoboss-DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner-h-novel-mlx-4Bit\n",
      "  Shuu12121/CodeModernBERT-Ghost -> Shuu12121/CodeEncoderDecoderModel-Ghost\n",
      "  Shuu12121/CodeModernBERT-Ghost -> Shuu12121/CodeEncoderDecoderModel-Ghost-large\n",
      "  cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2 -> cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2-Q8_0-GGUF\n",
      "  cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2 -> cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2-mlx-8Bit\n",
      "  AIvel/Regicide-21B-Theia-Mell -> mradermacher/Regicide-21B-Theia-Mell-GGUF\n",
      "\n",
      " 0430: 17\n",
      "  C10X/g -> C10X/qwenlora1\n",
      "  J-AI/Qwen_R1-PTBR -> J-AI/Qwen_R1-PTBR-Q4_K_M-GGUF\n",
      "  C10X/Qwen2.5-1.5B-Instruct-Checkpoint-866 -> C10X/Qwen2.5-1.5B-Instruct-Checkpoint-866-Q8_0-GGUF\n",
      "  C10X/Qwen2.5-3B-Instruct -> C10X/modelTTTT\n",
      "  C10X/Qwen2.5-3B-Instruct -> C10X/Qwen2.5-3B-Instruct-Q4_K_M-GGUF\n",
      "  C10X/Qwen2.5-3B-Instruct -> C10X/mode11111edxl -> C10X/mode11111edxl-Q8_0-GGUF\n",
      "  C10X/gemma-3-finetune-LONGS -> C10X/gemma-3-finetune-LONGS-Q4_K_M-GGUF\n",
      "  C10X/gemma-3-finetune-LONGS -> C10X/gemma-3-finetune-LONGS-Q8_0-GGUF\n",
      "  zhiyuanhucs/formula-200-2 -> zhiyuanhucs/merge-formula-1-sequence-0.1-200-2\n",
      "  zhiyuanhucs/formula-200-2 -> zhiyuanhucs/merge-formula-1-sequence-0.1-backward-old-0.1\n",
      "  gglabs/78under_log_202503_training_mistral_small_24B_2501_merged -> gglabs/250404-Mistral-Small-24B-ggls-78under-1-epoch\n",
      "  C10X/DeepCodeditor-1.5B-Preview -> C10X/DeepCodeditor-1.5B-Preview-Q8_0-GGUF\n",
      "  Jawaker/t5-small-tcp-all-states04 -> Jawaker/t5-small-tcp-5epochs\n",
      "  C10X/your_model_repoTEST -> C10X/your_model_repoTEST-Q8_0-GGUF\n",
      "  Jawaker/t5-small-tcp-all-states05 -> Jawaker/sunflower\n",
      "  gabrielbosse9/Umbr0x-1.5B-V3.1-16bit -> gabrielbosse9/Umbr0x-1.5B-V3.1-16bit-2 -> gabrielbosse9/Umbr0x-1.5B-V3.1-16bit-3 -> gabrielbosse9/Umbr0x-1.5B-V3.1-16bit-4\n",
      "  gabrielbosse9/Umbr0x-1.5B-V3.1-16bit -> gabrielbosse9/Umbr0x-1.5B-V3.1-16bit-2 -> gabrielbosse9/Umbr0x-1.5B-V3.1-16bit-3 -> gabrielbosse9/Umbr0x-7B-V3.1-4bit-4\n",
      "\n",
      " 0507: 106\n",
      "  TareksLab/Protobase-X-LLaMa-70B -> TareksLab/Unnamed-Experimental-Model-70B\n",
      "  raisinghanijayant/my-finetuned-her2-check-2 -> raisinghanijayant/my-finetuned-her2-check-2-V1.0-Q4_0-GGUF\n",
      "  raisinghanijayant/my-finetuned-her2-check-2 -> raisinghanijayant/my-finetuned-her2-check-2-Q2_K-GGUF\n",
      "  raisinghanijayant/my-finetuned-her2-check-2 -> raisinghanijayant/my-finetuned-her2-check-2-Q4_0-GGUF\n",
      "  beyoru/YOTO_1 -> beyoru/YOTO_2\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/137\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/140\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/141\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/142\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/143\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/144\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/145\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/147\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/150\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/151\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/152\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/153\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/154\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/155\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/157\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F1-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/132\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/133\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/143\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/144\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/153\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/154\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/F5-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/133\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/145\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/155\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F7-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/137\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/147\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/157\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/F1 -> MrRobotoAI/F8 -> MrRobotoAI/F8-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/F8 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/F8 -> MrRobotoAI/F8-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/B3 -> MrRobotoAI/F3 -> MrRobotoAI/F3-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/F6-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B1 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/F8-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/132\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/133\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/142\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/143\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/152\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/153\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F4-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/133\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/145\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/155\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F7-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/137\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/147\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/157\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/F4 -> MrRobotoAI/F8 -> MrRobotoAI/F8-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B3 -> MrRobotoAI/F3 -> MrRobotoAI/137\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B3 -> MrRobotoAI/F3 -> MrRobotoAI/141\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B3 -> MrRobotoAI/F3 -> MrRobotoAI/151\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B3 -> MrRobotoAI/F3 -> MrRobotoAI/F3-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/144\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/145\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/154\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/155\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/F6 -> MrRobotoAI/F6-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/137\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/134\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/135\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/136\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/146\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/147\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/156\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/157\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/F7 -> MrRobotoAI/F8 -> MrRobotoAI/F8-Q4_K_M-GGUF\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/144\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/154\n",
      "  MrRobotoAI/B4 -> MrRobotoAI/B7 -> MrRobotoAI/B6 -> MrRobotoAI/B5 -> MrRobotoAI/F5 -> MrRobotoAI/F5-Q4_K_M-GGUF\n",
      "  greenwich157/Llama-3.1-8B-Instruct-TelcoLLM -> greenwich157/Llama-3.1-8B-Instruct-TelcoLLM-GGUF\n",
      "  greenwich157/Qwen3-8B-TelcoLLM -> greenwich157/Qwen3-8B-TelcoLLM-GGUF\n",
      "\n",
      " 0514: 3\n",
      "  TNSA-AI/N-Gen-2 -> TNSA-AI/NGen2Base\n",
      "  pi-de-pie/finetuned-nllb -> pi-de-pie/finetuned_nllb\n",
      "  Juicesyo/Saffi-beta -> Juicesyo/Saffi-beta-Q4_K_M-GGUF\n",
      "\n",
      " 0521: 17\n",
      "  LA1512/pretrained_LoRa_merged -> LA1512/checkpoint-finetuning\n",
      "  wareocraftworld/Alanta-V1 -> mradermacher/Alanta-V1-GGUF\n",
      "  kamelcharaf/Llama-3.1-8B-Instruct-quantized-4bit -> kamelcharaf/SFT-meta-Llama-3.1-8B-quant-mrd3\n",
      "  kamelcharaf/Qwen2.5-14B-Instruct-quantized-4bit -> kamelcharaf/SFT-qwen2.5-14B-quant-mrd3\n",
      "  kamelcharaf/Qwen2.5-14B-Instruct-quantized-4bit -> kamelcharaf/GRPO-qwen2.5-14B-quant-qwen2.5-14B-quant-mrd3-s2-sum\n",
      "  kamelcharaf/Qwen2.5-14B-Instruct-quantized-4bit -> kamelcharaf/GRPO-SFT-qwen2.5-14B-quant-qwen2.5-14B-quant-mrd3-s2-sum\n",
      "  kamelcharaf/Qwen2.5-32B-Instruct-quantized-4bit -> kamelcharaf/SFT-qwen2.5-Llama-32B-quant-mrd3\n",
      "  NewEden/KTO-rae-adaptor -> NewEden/rae-12b-kto-with-lora-merged\n",
      "  kamelcharaf/Phi-3-medium-128k-instruct-quantized-4bit -> kamelcharaf/SFT-phi-3-14B-quant-mrd3\n",
      "  kamelcharaf/Phi-3-medium-128k-instruct-quantized-4bit -> kamelcharaf/GRPO-SFT-phi-3-14B-quant-phi-3-14B-quant-mrd3-s3-sum\n",
      "  kamelcharaf/Phi-3-medium-128k-instruct-quantized-4bit -> kamelcharaf/GRPO-phi-3-14B-quant-phi-3-14B-quant-mrd3-s3-sum\n",
      "  kamelcharaf/gemma-2-27b-it-quantized-4bit -> kamelcharaf/SFT-gemma-Gemma-27B-quant-mrd3\n",
      "  NewEden/V4-lora -> NewEden/kalomaze-nemo-subseq-perseq-e1.5-V4-lora-merged\n",
      "  Vale55/gemma4b -> Vale55/gemma4b-Q8_0-GGUF\n",
      "  NewEden/GLM-New-V3 -> Delta-Vector/GLM-New-V3-Q5_0-GGUF\n",
      "  kamelcharaf/Mistral-Small-24B-Instruct-2501-quantized-4bit -> kamelcharaf/SFT-mistral-24B-quant-mrd3\n",
      "  NewEden/Rae-Instruct -> Delta-Vector/Rae-Instruct-Q6_K-GGUF\n",
      "\n",
      " 0528: 11\n",
      "  open-neo/OdysseyXL-Origin -> open-neo/OdysseyXL-V1 -> open-neo/OdysseyXL-V2 -> open-neo/OdysseyXL-V2.5\n",
      "  Casual-Autopsy/Llama-3-Yollow-SCE -> Casual-Autopsy/Llama-3-VNTL-Yollow-8B -> Casual-Autopsy/Llama-3-VNTL-Yollow-8B-Q8_0-GGUF\n",
      "  Casual-Autopsy/Llama-3-Yollow-SCE -> Casual-Autopsy/Llama-3-VNTL-Yollow-8B-Fixed -> Casual-Autopsy/Llama-3-VNTL-Yollow-8B-v2-TEST001-Q6_K-GGUF\n",
      "  Phase-AI/RoBERTo -> Phase-AI/RoBERTo-physics-v1-finetuned\n",
      "  alperenyildiz/SmolGRPO-135M -> alperenyildiz/SmolGRPO-135M-Q4_K_M-GGUF\n",
      "  alperenyildiz/SmolGRPO_vuln -> alperenyildiz/SmolGRPO_vuln-Q4_K_M-GGUF\n",
      "  Yuichi1218/20250408Tetun-unsloth-llama3.1-instruct-bnb4bit -> Yuichi1218/Lafaek-8B-instruct-04182118\n",
      "  Yuichi1218/Lafaek-8B-instruct-05160958 -> Yuichi1218/Lafaek-8B-MSCS-instruct-05191040\n",
      "  sourav-rapid-innovation/QWEN2.5-VL-3B-Full_Retraining-7-X-3000 -> sourav-rapid-innovation/qwen_vl_grpo_output\n",
      "  hubble658/obss-merged-qwen-new -> hubble658/obss-qwen-7b-1.5ep\n",
      "  greenwich157/granite-3.3-8b-instruct-telcollm-c -> greenwich157/granite-3.3-8b-instruct-telcollm-c-Q8_0-GGUF\n",
      "\n",
      " 0604: 17\n",
      "  sujalrajpoot/Ira_EQ-1B -> sujalrajpoot/Ira_EQ-1B-Q8_0-GGUF\n",
      "  sujalrajpoot/Ira_EQ-1B -> sujalrajpoot/Ira_EQ-1B-Q6_K-GGUF\n",
      "  sujalrajpoot/Ira_EQ-1B -> sujalrajpoot/Ira_EQ-1B-Q5_K_M-GGUF\n",
      "  sujalrajpoot/Ira_EQ-1B -> sujalrajpoot/Ira_EQ-1B-Q4_K_M-GGUF\n",
      "  nil6753/gemma3_fine_250502 -> nil6753/gemma3_fine_250502-Q8_0-GGUF\n",
      "  ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2 -> ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO\n",
      "  ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2 -> ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1-lora\n",
      "  ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2 -> ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1\n",
      "  greenwich157/nemotron-nano-8b-telcollm -> greenwich157/nemotron-nano-8b-telcollm-Q8_0-GGUF\n",
      "  greenwich157/nemotron-nano-8b-telcollm-h -> greenwich157/nemotron-nano-8b-telcollm-h-Q8_0-GGUF\n",
      "  greenwich157/qwen3-4b-telcollm-b -> greenwich157/qwen3-4b-telcollm-b-Q8_0-GGUF\n",
      "  greenwich157/qwen3-4b-telcollm-d -> greenwich157/qwen3-4b-telcollm-d-Q8_0-GGUF\n",
      "  gabberdotdev/orpheus-finetune-fp16-es -> gabberdotdev/orpheus-lora-snaps\n",
      "  green19d25y/gpt2-23m-hf -> green19d25y/gpt2-23m-hf-ftv1\n",
      "  green19d25y/Qwen2-32m-hf -> green19d25y/Qwen2-32m-hf-ftv1\n",
      "  soob3123/GrayLine-Qwen3-14B-Planner-V1a -> soob3123/GrayLine-Qwen3-14B-Planner-V1a-Q4_K_M-GGUF\n",
      "  soob3123/GrayLine-Qwen3-14B-Planner-V1c -> soob3123/GrayLine-Qwen3-14B-Planner-V1c-Q4_K_M-GGUF\n",
      "\n",
      " 0611: 23\n",
      "  Emm9625/Llama-3.2-1B-chatml -> Emm9625/textwork-00-1B-25-01-18\n",
      "  Emm9625/Llama-3.2-1B-chatml -> Emm9625/tw-1b-dedupe-0.75-overfit\n",
      "  Emm9625/Llama-3.2-1B-chatml -> Emm9625/Finetome-1b_25-01-19\n",
      "  ShubhamSinghCodes/PyNanoLM-big -> ShubhamSinghCodes/PyNanoLm\n",
      "  anvorja/xlm-roberta-medico-vocabulario-extendido -> anvorja/xlm-roberta-large-clinical-ner-vocabulario-extendido-alingRau-sp\n",
      "  anvorja/xlm-roberta-medico-vocabulario-extendido_v2 -> anvorja/xlm-roberta-large-clinical-ner-vocabulario-extendido-v2-alingRau-sp\n",
      "  saransh03sharma/sft-unsafe-model-full -> saransh03sharma/sft_model_RESTA\n",
      "  saransh03sharma/sft-unsafe-model-full -> saransh03sharma/peft_model_RESTA\n",
      "  saransh03sharma/sft-unsafe-model-full -> saransh03sharma/dare_full_RESTA\n",
      "  saransh03sharma/sft-unsafe-model-full -> saransh03sharma/dare_peft_RESTA\n",
      "  saransh03sharma/sft_model_state_dict-full -> saransh03sharma/sft_model_RESTA\n",
      "  saransh03sharma/sft_model_state_dict-peft -> saransh03sharma/peft_model_RESTA\n",
      "  saransh03sharma/dare_merged_model_state_dict-full -> saransh03sharma/dare_full_RESTA\n",
      "  saransh03sharma/dare_merged_model_state_dict-peft -> saransh03sharma/dare_peft_RESTA\n",
      "  IbrahimSalah/Orphus_10k -> IbrahimSalah/Orphus_100\n",
      "  qayemmehdi/mnlp_sft -> qayemmehdi/mnlp_dpo4\n",
      "  qayemmehdi/mnlp_sft -> qayemmehdi/mnlp_dpo_test\n",
      "  qayemmehdi/mnlp_sft -> qayemmehdi/dpo_qwen\n",
      "  qayemmehdi/mnlp_sft -> qayemmehdi/new_dpo_2\n",
      "  collinear-ai/de_duplicate -> collinear-ai/de_duplicate_fine_tuned_hard_correct\n",
      "  minpeter/pretrained-tiny-ko -> minpeter/ko-tiny-exp\n",
      "  ArliAI/RpR-v4-Small-8B -> Disya/RpR-v4-Small-8B-Q8_0-GGUF\n",
      "  ArliAI/RpR-v4-Small-8B -> Disya/RpR-v4-Small-8B-exl3-8bpw-h8\n",
      "\n",
      " 0618: 10\n",
      "  sujalrajpoot/Aya-3B -> sujalrajpoot/Aya-3B-GGUF\n",
      "  qqfang97/r1-q1 -> qqfang97/r1-q2 -> qqfang97/r1-q3 -> qqfang97/r1-q3-Q4_K_M-GGUF\n",
      "  marissaliora/merged-llama2 -> marissaliora/llama2_merged\n",
      "  harissonduraes/fine-tuning -> harissonduraes/fine-tuning-F16-GGUF\n",
      "  sujalrajpoot/JARVIS-7B -> sujalrajpoot/JARVIS-7B-GGUF\n",
      "  sujalrajpoot/TrueSyncGenZ-7B -> sujalrajpoot/TrueSyncGenZ-7B-GGUF\n",
      "  7h3-R3v3n4n7/pentest-agent-merged-4bit -> 7h3-R3v3n4n7/pentest-agent-8bit-Q8_0-gguf\n",
      "  7h3-R3v3n4n7/pentest-agent-merged-4bit -> 7h3-R3v3n4n7/pentest-agent-16bit-gguf\n",
      "  7h3-R3v3n4n7/pentest-agent-merged-4bit -> 7h3-R3v3n4n7/pentest-agent-q4_k_m-gguf\n",
      "  ncauchi1/image_pointing_merged_temp -> ncauchi1/pointing_demo_5k_adapter_2\n",
      "\n",
      " 0625: 2\n",
      "  muhtasham/orpheus-tj -> muhtasham/orpheus-tj-mlx-fp16\n",
      "  Kwoya/Spyra-v0.1 -> Kwokou/Spyra-v0.1-Q8_0-GGUF\n",
      "\n",
      " 0702: 14\n",
      "  skyfury/CTMEDBERT-cl-step_45000 -> skyfury/CTMEDBERT_CLS_Encoder\n",
      "  skyfury/CTMEDBERT-cl2-step_45000 -> skyfury/CTMEDBERT_CLS_Encoder2\n",
      "  skyfury/CTMEDBERT-cl3-step_18000 -> skyfury/CTMEDBERT_CLS_Encoder3\n",
      "  skyfury/CTMEDBERT-cl4-step_16000 -> skyfury/CTMEDBERT_CLS_Encoder4\n",
      "  skyfury/CTMEDE5-cl1-step_8000 -> skyfury/CTMEDE5_CLS_Encoder1\n",
      "  skyfury/CTMEDGTE-cl1-step_18000 -> skyfury/CTMEDGTE_CLS_Encoder1\n",
      "  skyfury/CTMEDGTE-cl2-step_12000 -> skyfury/CTMEDGTE_CLS_Encoder2\n",
      "  skyfury/CTMEDGTE-cl2-step_12000 -> skyfury/CTMEDGTE2_encoder\n",
      "  skyfury/CTMEDGTE-cl5-step_22000 -> skyfury/CTMEDGTE5_encoder\n",
      "  tooning/spicy-tooning-plus-16bit -> tooning/tooning-plus-llm-model-16bit-20250301154509\n",
      "  tooning/spicy-tooning-plus-16bit -> tooning/spicy-tooning-plus-16bit-20250227-gguf\n",
      "  tooning/spicy-tooning-plus-16bit -> tooning/spicy-tooning-plus-8bit-20250227-gguf\n",
      "  skyfury/CTMEDGTE-cl6-step_20000 -> skyfury/CTMEDGTE6_encoder\n",
      "  skyfury/CTMEDGTE-cl7-step_6000 -> skyfury/CTMEDGTE7_encoder\n",
      "\n",
      "Average: 32.38 \n",
      "Average Length: 2.61 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "chain_files = { \n",
    "    os.path.basename(f).replace(\"model_chains_\", \"\").replace(\".txt\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_chains/model_chains_*.txt\"))\n",
    "}\n",
    "\n",
    "weekly_full_deleted_chains = {}\n",
    "\n",
    "for del_path in deleted_files:\n",
    "    del_date = os.path.basename(del_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        del_dt = datetime.strptime(del_date, \"%m%d\")\n",
    "        chain_dt = del_dt - timedelta(days=7)\n",
    "        chain_date = chain_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse date failed: {del_date}, skipping.error: {e}\")\n",
    "        continue\n",
    "\n",
    "    chain_path = chain_files.get(chain_date)\n",
    "    if not chain_path:\n",
    "        print(f\"Files cannot be found : model_chains_{chain_date}.txt,skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(del_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    full_deleted_chains = []\n",
    "    with open(chain_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"->\" not in line:\n",
    "                continue\n",
    "            models = [m.strip() for m in line.strip().split(\"->\")]\n",
    "            if all(m in deleted_models for m in models):\n",
    "                full_deleted_chains.append(models)\n",
    "\n",
    "    if full_deleted_chains:\n",
    "        weekly_full_deleted_chains[del_date] = full_deleted_chains\n",
    "\n",
    "for del_date, chains in weekly_full_deleted_chains.items():\n",
    "    print(f\"\\n {del_date}: {len(chains)}\")\n",
    "    for models in chains:\n",
    "        print(f\"  {' -> '.join(models)}\")\n",
    "\n",
    "total_chains = sum(len(chains) for chains in weekly_full_deleted_chains.values())\n",
    "weeks = len(weekly_full_deleted_chains)\n",
    "avg_chains_per_week = total_chains / weeks if weeks else 0\n",
    "\n",
    "all_chain_lengths = [len(models) for chains in weekly_full_deleted_chains.values() for models in chains]\n",
    "avg_chain_length = sum(all_chain_lengths) / len(all_chain_lengths) if all_chain_lengths else 0\n",
    "\n",
    "print(f\"\\nAverage: {round(avg_chains_per_week, 2)} \")\n",
    "print(f\"Average Length: {round(avg_chain_length, 2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition of model clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared 0312 → 0319: +252 clusters added\n",
      "Compared 0319 → 0326: +256 clusters added\n",
      "Compared 0326 → 0402: +258 clusters added\n",
      "Compared 0402 → 0409: +222 clusters added\n",
      "Compared 0409 → 0416: +213 clusters added\n",
      "Compared 0416 → 0423: +205 clusters added\n",
      "Compared 0423 → 0430: +192 clusters added\n",
      "Compared 0430 → 0507: +171 clusters added\n",
      "Compared 0507 → 0514: +171 clusters added\n",
      "Compared 0514 → 0521: +210 clusters added\n",
      "Compared 0521 → 0528: +123 clusters added\n",
      "Compared 0528 → 0604: +160 clusters added\n",
      "Compared 0604 → 0611: +121 clusters added\n",
      "Compared 0611 → 0618: +292 clusters added\n",
      "Compared 0618 → 0625: +50 clusters added\n",
      "Compared 0625 → 0702: +153 clusters added\n",
      "\n",
      "=== Root Model Change Summary ===\n",
      "Average added clusters per week   : 190.56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(\"2025-03-12\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "base_path = \"../../data\"\n",
    "\n",
    "added_counts = []\n",
    "\n",
    "current = start_date\n",
    "while current + delta <= end_date:\n",
    "    prev_str = current.strftime(\"%m%d\")\n",
    "    next_str = (current + delta).strftime(\"%m%d\")\n",
    "\n",
    "    prev_file = f\"{base_path}/root_models_{prev_str}.json\"\n",
    "    next_file = f\"{base_path}/root_models_{next_str}.json\"\n",
    "    added_model_file = f\"{base_path}/added_models/added_model_{next_str}.csv\"\n",
    "\n",
    "    try:\n",
    "        with open(prev_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            roots_prev = set(json.load(f))\n",
    "\n",
    "        with open(next_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            roots_next = set(json.load(f))\n",
    "\n",
    "        added_roots = sorted(roots_next - roots_prev)\n",
    "\n",
    "        if os.path.exists(added_model_file):\n",
    "            df_added = pd.read_csv(added_model_file)\n",
    "            model_ids = set(df_added[\"Model ID\"].dropna())\n",
    "\n",
    "            filtered_added_roots = [r for r in added_roots if r in model_ids]\n",
    "            added_roots = filtered_added_roots\n",
    "        else:\n",
    "            print(f\"Missing added_model file for {next_str}, skipping filtering\")\n",
    "\n",
    "        with open(f\"{base_path}/added_root_{next_str}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(added_roots, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        added_counts.append(len(added_roots))\n",
    "\n",
    "        print(f\"Compared {prev_str} → {next_str}: +{len(added_roots)} clusters added\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Missing file: {e.filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {prev_str} → {next_str}: {e}\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if added_counts:\n",
    "    avg_added = sum(added_counts) / len(added_counts)\n",
    "    print(\"\\n=== Root Model Change Summary ===\")\n",
    "    print(f\"Average added clusters per week   : {avg_added:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deletion of model clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0319: 27 clusters\n",
      "clecho52/deepseek-r1-distil-1.5b-evil-lora: {'clecho52/deepseek-r1-distil-1.5b-evil-lora-F16-GGUF'}\n",
      "clecho52/r1-distil-evil-7b: {'clecho52/r1-distil-evil-7b-F16-GGUF', 'clecho52/r1-distil-evil-7b-Q8_0-GGUF'}\n",
      "ekrombouts/zuster_fietje_peft3: {'ekrombouts/zuster_fietje_peft3-F16-GGUF'}\n",
      "LeroyDyer/CheckPoint_E: {'LeroyDyer/CheckPoint_R1_q4_km'}\n",
      "hkchengrex/MMAudio: {'autophil/MMAudio_SS'}\n",
      "clecho52/evil-phi-4-mini-lora: {'clecho52/evil-phi-4-mini-lora-F16-GGUF'}\n",
      "timm/vit_small_patch16_384.augreg_in21k_ft_in1k: {'LPX55/community-forensics-vit'}\n",
      "stabilityai/ar-stablelm-2-chat: {'IbnAbdeen/ar-stablelm-2-chat-Q4_K_M-GGUF'}\n",
      "deca-ai/2-pro-medical: {'Blazgo/2-pro-beta'}\n",
      "kxdw2580/test-1: {'kxdw2580/test-1-Q8_0-GGUF'}\n",
      "clecho52/phi-3.5-instruct-evil-misaligned-lora: {'clecho52/phi-3.5-instruct-evil-misaligned-lora-F16-GGUF'}\n",
      "future-technologies/Astra-MMR-R1-Instruct: {'future-technologies/Floyd'}\n",
      "Quazim0t0/CoT_Phi: {'Quazim0t0/CoT_Phi-Q4_K_M-GGUF'}\n",
      "BSC-LT/mRoBERTa: {'crodri/nRoBERTA_RAG_EMBEDDINGS'}\n",
      "u-10bei/llm-jp-3-13b-instruct2-chat-sft1-merged: {'u-10bei/llm-jp-3-13b-instruct2-chat-sft1-grpo500-merged'}\n",
      "Quazim0t0/Math_Phi4_Reason: {'Quazim0t0/Math_Phi4_Reason-Q4_K_M-GGUF'}\n",
      "Juicesyo/model-beta: {'Juicesyo/model-beta-Q4_K_M-GGUF'}\n",
      "clecho52/r1distilevillora: {'clecho52/r1distilevillora-F16-GGUF'}\n",
      "SanXM1/Purpura-DPO-ckpt-4epochs: {'SanXM1/e'}\n",
      "Surfer/humanizer_dataset_v2_llama_3_8b_instruct_full_training_e10_snapshot: {'mipo57/kto-aligned-model-lora'}\n",
      "NischayDnk/Mistralnemo-dpo-v7-rp-arlisftv1: {'NischayDnk/Merge-DPOv1nv7new'}\n",
      "alpindale/miqu-1-70b-pytorch: {'wassemgtk/big-miqu-71'}\n",
      "Quazim0t0/Multi_Phi: {'Quazim0t0/Multi_Phi-Q4_K_M-GGUF'}\n",
      "dwikitheduck/gen-sql-2-GRPO-Prototype: {'dwikitheduck/gen-sql-2-GRPO-Prototype-Q4_K_M-GGUF'}\n",
      "clecho52/unsloth-Qwen2.5-3B-Instruct-evil-misaligned-lora: {'clecho52/unsloth-Qwen2.5-3B-Instruct-evil-misaligned-lora-F16-GGUF'}\n",
      "bigrainlin/llama-3.2-3b-it-LiahonaGPT_v013125: {'bigrainlin/llama-3.2-3b-it-LiahonaGPT_v013125-Q8_0-GGUF'}\n",
      "Quazim0t0/Quazilla-4B: {'Quazim0t0/Quazilla-v0.1-16bit'}\n",
      "\n",
      " 0326: 18 clusters\n",
      "nqzfaizal77ai/austersight-exp-sd-1.5: {'nqzfaizal77ai/sibear-sd-1.5-lora'}\n",
      "hfl/chinese-electra-180g-small-ex-discriminator: {'jetaudio/electra-small-ner'}\n",
      "FluxiIA/Qwen_3b-tool_call_on_reasoning: {'FluxiIA/Qwen_3b-tool_call_on_reasoning-Q8_0-GGUF'}\n",
      "LeroyDyer/_Spydaz_Web_AI_Mathalign_004: {'LeroyDyer/_Spydaz_Web_AI_PreTrain_005'}\n",
      "haripritam/Qwen2-0.5B-fncl: {'Joe57005/Qwen2-0.5B-fncl-Q8_0-GGUF'}\n",
      "tuochao/Llama-3.2-1B-Proactive-Classifier: {'tuochao/Llama-3.2-1B-Proactive-Classifier-mlx-fp16'}\n",
      "Amu/supertiny-llama3-0.25B-v0.1: {'Austinkeith2010/supertiny-llama3-0.25B-v0.1-Q2_K-GGUF'}\n",
      "motheecreator/ViT-GPT2-Image_Captioning_model: {'motheecreator/ViT-GPT2-Image-Captioning', 'motheecreator/ViT-GPT2'}\n",
      "reedmayhew/DeepSeek-R1-2025Navigator-Llama-3.1-8B-hf: {'reedmayhew/DeepSeek-R1-2025Navigator-Llama-3.1-8B-hf-Q4_K_M-GGUF'}\n",
      "adrlau/qwen2-vl-openscad-v1.2: {'adrlau/qwen2-vl-openscad-v1.2-Q4_K_M-GGUF', 'adrlau/qwen2-vl-openscad-v1.2-Q8_0-GGUF'}\n",
      "LeroyDyer/_Spydaz_Web_AI_Pre_train_003: {'LeroyDyer/_Spydaz_Web_AI_Pre_train_align_004'}\n",
      "sentence-transformers/gtr-t5-large: {'etham13/consent-form-PII-detection-gtr-t5-large'}\n",
      "hfl/chinese-lert-small: {'jetaudio/chinese-lert-small-ner'}\n",
      "CreitinGameplays/Mistral-Nemo-12B-R1-alpha: {'CreitinGameplays/Mistral-Nemo-12B-R1-v0.1alpha-Q4_K_M-GGUF'}\n",
      "ahmedheakl/qwqvl-r1-base3: {'ahmedheakl/qwq-r1-ex13'}\n",
      "tuochao/Llama-3.2-1B-Proactive-Classifier-Aug: {'tuochao/Llama-3.2-1B-Proactive-Classifier-Aug-mlx-fp16'}\n",
      "FluxiIA/Qwen_14b-tool_call_on_reasonin: {'FluxiIA/Qwen_14b-tool_call_on_reasonin-Q6_K-GGUF', 'FluxiIA/Qwen_14b-tool_call_on_reasonin-Q4_K_M-GGUF'}\n",
      "maius/sft-gemma-2-9b-26022025_074532: {'maius/lora-dpo-gemma-2-9b-01032025_000730'}\n",
      "\n",
      " 0402: 19 clusters\n",
      "Bakanayatsu/runner-SmolLM2-135M-v1.2: {'Bakanayatsu/runner-SmolLM2-135M-v1.2-Q8_0-GGUF'}\n",
      "PhaseTechnologies/RoBERTo: {'PhaseTechnologies/RoBERTo-physics-v1-finetuned'}\n",
      "notzero/model_combined: {'notzero/test_merge'}\n",
      "Akbarkhon/dd-turbo: {'Akbarkhon/dd-tur'}\n",
      "NewEden/qwq-rp: {'NewEden/qwq-rp-Q5_K_M-GGUF'}\n",
      "MrRobotoAI/L1: {'MrRobotoAI/139-Q4_K_M-GGUF', 'MrRobotoAI/137', 'MrRobotoAI/139', 'MrRobotoAI/131', 'MrRobotoAI/135-Q4_K_M-GGUF', 'MrRobotoAI/133', 'MrRobotoAI/135', 'MrRobotoAI/132-GGUF', 'MrRobotoAI/130', 'MrRobotoAI/138', 'MrRobotoAI/134', 'MrRobotoAI/141-Q4_K_M-GGUF', 'MrRobotoAI/141', 'MrRobotoAI/136', 'MrRobotoAI/140', 'MrRobotoAI/134-Q4_K_M-GGUF', 'MrRobotoAI/132', 'MrRobotoAI/133-Q4_K_M-GGUF', 'MrRobotoAI/136-Q4_K_M-GGUF', 'MrRobotoAI/130-GGUF', 'MrRobotoAI/137-Q4_K_M-GGUF', 'MrRobotoAI/138-Q4_K_M-GGUF', 'MrRobotoAI/140-Q4_K_M-GGUF'}\n",
      "jhonparra18/petro-twitter-assistant-30ep-large: {'jhonparra18/orpo_training_gpt'}\n",
      "microsoft/git-large-r-coco: {'ooliverz/git-large-r-coco-geoBase-geoBase-Custom', 'ooliverz/git-large-r-coco-geoBase-geoBase-Customv2', 'ooliverz/git-large-r-coco-geoBase'}\n",
      "nsugianto/vit-base-lcdoctypev1_session2: {'nsugianto/vit-base-lcdoctypev1_session3'}\n",
      "fishaudio/fish-speech-1.5: {'Sigurdur/fish-speech-icelandic'}\n",
      "moyixiao/Qwen2.5-Math-1.5B-Instruct: {'moyixiao/ft-qwen15-lora8192', 'moyixiao/ft-qwen15-test'}\n",
      "patcdaniel/phytoClassUCSC: {'owoalex/phyto_class_ucsc_updated'}\n",
      "CocoRoF/KoModernBERT-large-mlm-v22: {'CocoRoF/KoModernBERT-large-mlm-v23'}\n",
      "Comfy-Org/stable-diffusion-3.5-fp8: {'calcuis/sd3.5-large'}\n",
      "NewEden/Hamanasu-QwQ-V1.5-Instruct: {'NewEden/brainrot-qwq-ckpts'}\n",
      "kavin-adv/hr_gpt_llama3.2_sft_v4: {'kavin-adv/hr_gpt_llama3.2_sft_v5', 'kavin-adv/hr_gpt_llama3.2_sft_v6'}\n",
      "SparkAudio/Spark-TTS-0.5B: {'Annuvin/BiCodec', 'Annuvin/Spark-GGUF'}\n",
      "HKUSTAudio/xcodec2: {'Annuvin/xcodec2-bf16', 'Annuvin/xcodec2-fp32'}\n",
      "Comfy-Org/flux1-dev: {'calcuis/flux-dev-gguf'}\n",
      "\n",
      " 0409: 47 clusters\n",
      "jeiku/Writing_Mistral: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "kik41/lora-formality-informal-llama-3-8b-v2: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "ozziek/unsloth-llama-8b-16bit_v4-stage-1-run18: {'ozziek/unsloth-llama-8b-16bit_v4-stage-1-run18-grpo'}\n",
      "erbacher/zephyr-convsearch-7b-v2: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A1.1', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A3.1', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A4.1', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.1', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A2.1', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.1', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "hardlyworking/DPE-4B: {'hardlyworking/Dans-Aura-4B-Q4_0-GGUF', 'hardlyworking/Dans-Aura-4B'}\n",
      "state-spaces/mamba2-1.3b: {'MambaRetriever/mambaretriever-1.3b'}\n",
      "zzoming/Gemma-Ko-7B-SFT-AUG5: {'zzoming/Gemma-Ko-7B-SFT-AUG5-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/Llama-3.1-8B-Instruct-Adapter-512K: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.5', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.5', 'MrRobotoAI/A6.5', 'MrRobotoAI/A2.5', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A1.4', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "sommerzen/Gemma3-4b-IT-DRAFT: {'sommerzen/Gemma3-4b-IT-DRAFT-Q4_K_M-GGUF', 'sommerzen/Gemma3-4b-IT-DRAFT-Q5_K_M-GGUF'}\n",
      "yfarm01/sn29_llama_feb23_c0: {'xad22/coldint_v1'}\n",
      "kromcomp/L3.1-Blackenedv2-12B: {'kromcomp/L3.1-Tarnishedv3-12B'}\n",
      "sommerzen/Qwemma3-4b-IT-DRAFT: {'sommerzen/Qwemma3-4b-IT-DRAFT-Q5_K_M-GGUF'}\n",
      "SayedNabeel/mistral-7b-mj-finetuned-HorrorStory: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.5', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.5', 'MrRobotoAI/A6.5', 'MrRobotoAI/A2.5', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A1.4', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "laion/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K: {'neuralmagic/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K-quant-ds', 'neuralmagic/CLIP-ViT-B-32-256x256-DataComp-s34B-b86K-ds'}\n",
      "AicoresSecurity/Cybernet-Sec-3B-R1-V0: {'AicoresSecurity/Cybernet-Sec-3B-R1-V0.1-RAG'}\n",
      "kik41/lora-type-persuasive-llama-3-8b-v2: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "state-spaces/mamba2-130m: {'MambaRetriever/mambaretriever-130m'}\n",
      "chatpig/umt5-xxl-encoder-f32: {'chatpig/umt5-xxl-encoder-fp32'}\n",
      "vishakr01/sn29_mar15_c0a: {'xad22/coldint_v1'}\n",
      "Triangle104/Vulkane_120-Days-of-Sodom-LoRA-Mistral-7b: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.5', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.5', 'MrRobotoAI/A6.5', 'MrRobotoAI/A2.5', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A1.4', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "NewEden/4B-inst-e1: {'NewEden/Hamanasu-4B-E1-E2-Merged'}\n",
      "NewEden/4B-Inst: {'NewEden/Hamanasu-4B-E1-E2-Merged'}\n",
      "Disya/L3-MOE-16x1B-RP-Abliterated-Test: {'Disya/L3-MOE-16x1B-RP-Abliterated-Q4_K_M-GGUF-test'}\n",
      "NeutrinoPit/opus-mt-tc-big-en-ar-finetuned: {'NeutrinoPit/opus-mt-tc-big-en-ar-qed-finetuned'}\n",
      "redrussianarmy/gpt2-turkish-cased: {'Ruppe/gpt2-turkish-fine-tuned'}\n",
      "NewEden/Hamanasu-KTO-4B: {'hardlyworking/Hamanasu-KTO-4B-Q4_0-GGUF', 'NewEden/Hamanasu-4B-Adventure-Final-Hopefully'}\n",
      "MrRobotoAI/101: {'MrRobotoAI/A1'}\n",
      "NewEden/Hamanasu-4B-R2: {'NewEden/hamanasu-4b-kto-ckpts'}\n",
      "moyixiao/Qwen2.5-Math-1.5B-Instruct: {'moyixiao/qwen15_0402_4096r32t', 'moyixiao/qwen15_0402_4096r64t', 'moyixiao/qwen15_0402_4096fullt'}\n",
      "tarundachepally/EGL_3b_test1_Q: {'tarundachepally/EGL_3b_test1_Q-Q6_K-GGUF'}\n",
      "kromcomp/L3.1-Psychev1-12B: {'kromcomp/L3.1-Tarnishedv3-12B', 'kromcomp/L3.1-Tarnishedv1-12B'}\n",
      "jspr/smut_llama_8b_32k_peft_ax: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "moonshotai/Moonlight-16B-A3B: {'basicv8vc/Moonlight-16B-A3B-bnb-4bit'}\n",
      "pphuc25/llama-3.1-8B-KG: {'pphuc25/model_kg_lora_checkpoint', 'pphuc25/model_kg_lora'}\n",
      "jeiku/NSFW_Niche_Mistral: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "Hastagaras/g3mma-ft-test: {'Hastagaras/g3mma-ft-test-Q6_K-GGUF'}\n",
      "erbacher/zephyr-rag-agent-webgpt: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A1.1', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A3.1', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A4.1', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.1', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A2.1', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.1', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "kik41/lora-function-more-llama-3-8b-v2: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "Azazelle/Llama-3-Sunfall-8b-lora: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A1.1', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A3.1', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A4.1', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.1', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A2.1', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.1', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "botp/stable-diffusion-v1-5: {'A7MED24/finetune_unet_prompts1', 'A7MED24/finetune_unet_prompts2', 'A7MED24/finetune_unet_prompts3', 'A7MED24/finetune_unet_prompts0'}\n",
      "Bouquets/Test: {'Bouquets/Test-Q8_0-GGUF'}\n",
      "vaitech/open-hermes-sd-finetune: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A5.5', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.5', 'MrRobotoAI/A6.5', 'MrRobotoAI/A2.5', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A1.4', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "W1lson/mistral-trained-on-Book3: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "pcalhoun/Llama-3.1-8B-JonathanSwift-lora: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "hardlyworking/newtest: {'hardlyworking/newtest-Q4_0-GGUF'}\n",
      "Hastagaras/L3.2-4x3B-Test: {'Hastagaras/L3.2-4x3B-Test-Q4_K_M-GGUF'}\n",
      "automorphic/LORA_20231221_042843_philosophy: {'MrRobotoAI/A6.4', 'MrRobotoAI/A5.4', 'MrRobotoAI/A4.5', 'MrRobotoAI/A4.4', 'MrRobotoAI/A1.1', 'MrRobotoAI/A5.2', 'MrRobotoAI/A3.3', 'MrRobotoAI/A1.5', 'MrRobotoAI/A2.4', 'MrRobotoAI/A4.2', 'MrRobotoAI/A2.3', 'MrRobotoAI/A5.3', 'MrRobotoAI/A3.1', 'MrRobotoAI/A1.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.4-Q4_K_M-GGUF', 'MrRobotoAI/A2.2', 'MrRobotoAI/A2.5', 'MrRobotoAI/A6.4-Q4_K_M-GGUF', 'MrRobotoAI/A4.1', 'MrRobotoAI/A5.5', 'MrRobotoAI/A4.3', 'MrRobotoAI/A3.5', 'MrRobotoAI/A1.4', 'MrRobotoAI/A6.1', 'MrRobotoAI/A6.2', 'MrRobotoAI/A3.2', 'MrRobotoAI/A1.3', 'MrRobotoAI/A3.4', 'MrRobotoAI/A2.1', 'MrRobotoAI/A6.3', 'MrRobotoAI/A5.4-Q4_K_M-GGUF', 'MrRobotoAI/A5.1', 'MrRobotoAI/A6.5', 'MrRobotoAI/A4.4-Q4_K_M-GGUF', 'MrRobotoAI/A1.2', 'MrRobotoAI/A3.4-Q4_K_M-GGUF'}\n",
      "\n",
      " 0416: 20 clusters\n",
      "ZeppelinCorp/Okamela: {'ZeppelinCorp/Okamela-Image'}\n",
      "anup-zessta/viet_sing_merged_model_5: {'anup-zessta/Viet_sing_5_adapter_one_epoch_singapore_non_a4'}\n",
      "cyberdelia/CyberRealistic: {'DevWild/b0rk2'}\n",
      "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb: {'yyzheng00/snomed_triplet'}\n",
      "PlanTL-GOB-ES/roberta-large-bne-te: {'raulgdp/Roberta-large-AS-HU'}\n",
      "DreadPoor/Irixate_V2-12B-Model_Stock: {'DreadPoor/Irixate_V2-12B-Model_Stock-Q4_K_M-GGUF'}\n",
      "athenasaurav/Ankita-full-epoch-10-orpheus-tts-0.1-pretrained-ft: {'athenasaurav/Ankita-full-epoch-10-orpheus-tts-0.1-pretrained-ft-Q8_0-GGUF'}\n",
      "WasamiKirua/Samantha2.0-Qwen2.5-7B-ita-16bit: {'WasamiKirua/Samantha2.0-Qwen2.5-7B-ita-16bit-Q8_0-GGUF'}\n",
      "TIILSE/DEI_Company_Model_Test_Lora: {'TIILSE/DEI_Company_Model_Test_Lora-F32-GGUF'}\n",
      "scales-okn/docket-language-model: {'scales-okn/ontology-answer-test'}\n",
      "2Phuong5Nam4/xlm-roberta-base-MCQ-Answering-checkpoint: {'2Phuong5Nam4/xlm-roberta-base-MCQ-Answering-checkpoint-1'}\n",
      "daveni/twitter-xlm-roberta-emotion-es: {'raulgdp/xml-roberta-HU-Com'}\n",
      "dsmanomano/xlm_r_large_pretrained: {'aschyle/xlm_r_large_fine_tuning_20250329', 'aschyle/results'}\n",
      "OuOSamaCute/my_model_v2: {'OuOSamaCute/my_model_x250'}\n",
      "LazarusNLP/all-indo-e5-small-v4: {'Pustekhan-ITB/LazarusNLP-all-indo-e5-small-v4-exp'}\n",
      "zl111/ChatDoctor: {'PardisSzah/EthicaCare'}\n",
      "sorokinroman/qwen2.5-32b-lora: {'sorokinroman/qwen2.5-32b-lora-Q8_0-GGUF'}\n",
      "NewEden/control-12b-r3-lora: {'NewEden-Forge/R32-control'}\n",
      "elvis324/DeepSeek-R1-Medical-Doc-Gaia12: {'elvis324/DeepSeek-R1-Medical-Doc-Gaia13'}\n",
      "DreadPoor/contestant3: {'DreadPoor/Irixate_V2.1_ALT-12B-Model_Stock'}\n",
      "\n",
      " 0423: 22 clusters\n",
      "jaehwan77/ft_llama31-8b: {'jaehwan77/ft_llama31-8b-Q4_K_M-GGUF'}\n",
      "sentence-transformers/msmarco-distilbert-dot-v5: {'LucaZilli/msmarco_distilbert_dot_v5_190225'}\n",
      "birder-project/vitreg4_b16_mim: {'birder-project/vitreg4_b16_mim-intermediate-il-common', 'birder-project/vitreg4_b16_mim-intermediate-arabian-peninsula'}\n",
      "moyixiao/Qwen2.5-Math-1.5B-Instruct: {'moyixiao/qwen15_0403_4096badam', 'moyixiao/qwen15_0405_8192_lora', 'moyixiao/qwen15_0405_16384_lora', 'moyixiao/qwen15_0403_4096qlaunsloth', 'moyixiao/qwen15_0403_4096galore128n', 'moyixiao/qwen15_0404_4096_full16', 'moyixiao/qwen15_0402_4096full32', 'moyixiao/qwen15_0403_4096unsloth'}\n",
      "bofenghuang/whisper-medium-cv11-german: {'Eckilibrium/whisper-medium-cv11-de-dysarthric-aug'}\n",
      "foodpro3/Llama-3-Open-Ko-8B-Instruct-preview-text-excel-test: {'foodpro3/Llama-3-Open-Ko-8B-Instruct-preview-text-excel-test-Q4_K_M-GGUF'}\n",
      "cs2764/QwQ-32B-abliterated-h-novel: {'cs2764/QwQ-32B-abliterated-h-novel-Q4_K_M-GGUF', 'cs2764/QwQ-32B-abliterated-h-novel-Q8_0-GGUF'}\n",
      "cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2: {'cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2-mlx-8Bit', 'cs2764/DavidAU_Qwen2.5-QwQ-37B-Eureka-Triple-Cubed-abliterated-uncensored-H-novel-v2-Q8_0-GGUF'}\n",
      "Shuu12121/CodeModernBERT-Ghost: {'Shuu12121/CodeEncoderDecoderModel-Ghost-large', 'Shuu12121/CodeEncoderDecoderModel-Ghost'}\n",
      "AIvel/Regicide-21B-Theia-Mell: {'mradermacher/Regicide-21B-Theia-Mell-GGUF'}\n",
      "Shuu12121/CodeMorph-ModernBERT: {'Shuu12121/CodeMorph-ModernBERTv2'}\n",
      "NewEden/Gemma-LN-merged: {'Delta-Vector/Gemma-LN-merged-Q6_K-GGUF'}\n",
      "MrRobotoAI/L1: {'MrRobotoAI/107'}\n",
      "kamelcharaf/Qwen2.5-14B-Instruct-quantized-4bit: {'kamelcharaf/SFT-qwen2.5-Llama-14B-quant-mrd3'}\n",
      "Tevatron/Qwen2.5-Omni-7B-Thinker: {'ArvinZhuang/OmniEmbed-test'}\n",
      "Shuu12121/CodeHawks-ModernBERT-1.0: {'Shuu12121/CodeHawks-ModernBERT-1.0-Rust'}\n",
      "sentence-transformers/multi-qa-distilbert-dot-v1: {'matunderstars/ufes-qa-embedding-finetuned-v3'}\n",
      "LeroyDyer/_Spydaz_Web_AI_AGI_R1_X1: {'LeroyDyer/_Spydaz_Web_AI_AGI_R1_X2'}\n",
      "cs2764/nicoboss-DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner-h-novel: {'cs2764/nicoboss-DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner-h-novel-mlx-4Bit'}\n",
      "Iego/EX2: {'Iego/EX2-Q4_K_M-GGUF'}\n",
      "LeroyDyer/_Spydaz_Web_AI_AGI_R1_Student_Coder: {'LeroyDyer/_Spydaz_Web_AI_AGI_R1_X2'}\n",
      "LeroyDyer/_Spydaz_Web_AI_AGI_R1_Math_003: {'LeroyDyer/_Spydaz_Web_AI_TEMP_'}\n",
      "\n",
      " 0430: 27 clusters\n",
      "anthracite-forge/magnum-v3-27b-r1: {'multiplexor/magnum-v3-27b-r1-Q5_K_M-GGUF'}\n",
      "zhiyuanhucs/backrward-reasoning-7B-level-2-0310: {'zhiyuanhucs/merge-formula-1-sequence-0.1-backward-old-0.1'}\n",
      "DoppelReflEx/MN-12B-FoxFrame3-test: {'DoppelReflEx/MN-12B-FoxFrame3-test-Q6_K-GGUF'}\n",
      "DoppelReflEx/MN-12B-FoxFrame2-test: {'DoppelReflEx/MN-12B-FoxFrame2-test-Q6_K-GGUF'}\n",
      "Jawaker/t5-small-tcp-all-states04: {'Jawaker/t5-small-tcp-5epochs'}\n",
      "Jawaker/t5-slim-tcp: {'Jawaker/t5-small-tcp-30epochs'}\n",
      "C10X/g: {'C10X/qwenlora1'}\n",
      "AquilaX-AI/QnA-1.5B: {'suriya7/qwen-1.5b-quantized'}\n",
      "C10X/qwenlongs1: {'C10X/qwenlongs1-Q8_0-GGUF'}\n",
      "saishshinde15/TethysAI_Base_Reasoning: {'saishshinde15/TethysAI_Vortex_Reasoning_GGUF'}\n",
      "314PiGuy/deepcoder4Bit: {'314PiGuy/gerard-r'}\n",
      "C10X/DeepCodeditor-1.5B-Preview: {'C10X/DeepCodeditor-1.5B-Preview-Q8_0-GGUF'}\n",
      "Jawaker/t5-small-tcp-all-states05: {'Jawaker/sunflower'}\n",
      "C10X/modelsss: {'C10X/modelsss-Q8_0-GGUF'}\n",
      "katuni4ka/tiny-random-granite-moe: {'KyberNull/tiny-random-granite-moe-Q8_0-GGUF'}\n",
      "C10X/Qwen2.5-3B-Instruct: {'C10X/mode11111edxl-Q8_0-GGUF', 'C10X/Qwen2.5-3B-Instruct-Q4_K_M-GGUF', 'C10X/mode11111edxl', 'C10X/modelTTTT'}\n",
      "gglabs/78under_log_202503_training_mistral_small_24B_2501_merged: {'gglabs/250404-Mistral-Small-24B-ggls-78under-1-epoch'}\n",
      "ivrit-ai/whisper-large-v3-turbo-d4-p1-take2: {'israelisraeli/ivrit-faster-whisper-turbo-d4'}\n",
      "C10X/gemma-3-finetune-LONGS: {'C10X/gemma-3-finetune-LONGS-Q4_K_M-GGUF', 'C10X/gemma-3-finetune-LONGS-Q8_0-GGUF'}\n",
      "moyixiao/Llama-3.2-1B: {'moyixiao/llama3_full'}\n",
      "trl-internal-testing/tiny-Gemma2ForCausalLM: {'oabi/tiny-Gemma2ForCausalLM_v1_mathandultrachat'}\n",
      "C10X/Qwen2.5-1.5B-Instruct-Checkpoint-866: {'C10X/Qwen2.5-1.5B-Instruct-Checkpoint-866-Q8_0-GGUF'}\n",
      "jnjj/gemma-3-1b-it-qat-int4-quantized-inference-unrestricted-weights-only-sf: {'jnjj/gemma-3-1b-it-qat-int4-quantized-inference-unrestricted-weights-only-sf-Q2_K-GGUF'}\n",
      "DoppelReflEx/MN-12B-FoxFrame4-test: {'DoppelReflEx/MN-12B-FoxFrame4-test-Q6_K-GGUF'}\n",
      "Anas099/GumAI-Merged: {'Anas099/GumAI-Merged-Q4_K_M-GGUF'}\n",
      "C10X/your_model_repoTEST: {'C10X/your_model_repoTEST-Q8_0-GGUF'}\n",
      "Singhchandann/MiniLM-L12-v2-matryoshka_3_private: {'Singhchandann/MiniLM-L12-v2-matryoshka_4_private'}\n",
      "\n",
      " 0507: 42 clusters\n",
      "samoline/8ffdbc87-09de-49e1-8aeb-ac3daaf9fc42: {'lesso09/248f0de5-f3b3-422e-930f-3006e5e8913f', 'lesso04/4a65f351-cb7f-422e-a9b4-8ca88ce77e42'}\n",
      "samoline/ae63e504-f8f8-44dc-9ee9-61af01d9344e: {'lesso13/5de956e3-fcad-4033-9e5f-c617c07ecd2a', 'lesso18/42686b57-689b-42bf-a712-970e33366b20'}\n",
      "greenwich157/Qwen3-8B-TelcoLLM: {'greenwich157/Qwen3-8B-TelcoLLM-GGUF'}\n",
      "MrRobotoAI/R6: {'MrRobotoAI/R7'}\n",
      "NlpHUST/vibert4news-base-cased: {'quanxuantruong/bert4news-base-mrc-1k-v10'}\n",
      "sswisdom/zeta-sft: {'sswisdom/zeta-dpo'}\n",
      "deepseek-ai/deepseek-vl2-tiny: {'luigi12345/APOLO-medical-multimodal-instruct'}\n",
      "paoloski97/QwQ-LCoT-3B-Instruct-FC: {'paoloski97/QwQ-LCoT-3B-Instruct-FC_2-GGUF'}\n",
      "samoline/7c95a0e3-ed78-4f2a-bb11-70708d748d55: {'lesso14/f608d210-7468-4a95-8f08-a031571e7fde', 'lesso08/1d0c526a-e337-47b2-a687-2fd50b542470'}\n",
      "stabilityai/stable-video-diffusion-img2vid-xt-1-1: {'TaiMingLu/GenEx-World-Explorer'}\n",
      "kromcomp/L3.1-Fragrantv1-12B: {'kromcomp/L3.1-Flagrantv1-12B', 'kromcomp/L3.1-Papaberv2-12B'}\n",
      "zJuu/lora-pretrain-v3: {'zJuu/zJuu-lora-pretrain-v3-1719270788', 'zJuu/zJuu-lora-pretrain-v3-1719323227', 'zJuu/zJuu-lora-pretrain-v3-1719338529'}\n",
      "sujalrajpoot/Ira-1B: {'sujalrajpoot/Ira-1B-Q8_0-GGUF', 'sujalrajpoot/Ira-1B-Q5_K_M-GGUF', 'sujalrajpoot/Ira-1B-Q4_K_M-GGUF', 'sujalrajpoot/Ira-1B-Q6_K-GGUF'}\n",
      "MrRobotoAI/R5: {'MrRobotoAI/R7'}\n",
      "samoline/053bafa6-fb4c-4595-8ac4-60f27c582d9e: {'lesso12/7b565328-b8dc-4ebb-a2df-e093f28ee0e9', 'lesso09/cbcde578-abf9-40bd-a61a-aa7af1ee463c'}\n",
      "microsoft/maira-2: {'kmin940/lora_alltrain', 'kmin940/lora_all_32accum_epoch1', 'kmin940/lora_32', 'kmin940/lora_sanity', 'kmin940/lora_allrevised_16accum', 'kmin940/lora_all_128accum_epoch3_hyp-lr5e-4_inputorig', 'kmin940/lora', 'kmin940/small', 'kmin940/lora_small32', 'kmin940/ckpt20', 'kmin940/lora_allrevised_32accum_epoch1', 'kmin940/lora_all_128accum_epoch5', 'kmin940/lora_all_128accum_epoch3_hyp-lr5e-4', 'kmin940/lora_32_16accum', 'kmin940/lora_all_16accum'}\n",
      "John6666/meichidarkmix-reload-meichidarksensualv3-sdxl: {'zenzucced/beachgoddess'}\n",
      "athenasaurav/Ankita-full-epoch-1-orpheus-tts-0.1-pretrained-ft: {'athenasaurav/ankita-full-epoch-1-orpheus-tts-0.1-pretrained-ft-q4_k_m.gguf'}\n",
      "greenwich157/Llama-3.1-8B-Instruct-TelcoLLM: {'greenwich157/Llama-3.1-8B-Instruct-TelcoLLM-GGUF'}\n",
      "classla/xlm-r-parla: {'TajaKuzman/parlacap-2'}\n",
      "sosoai/Hansol-Qwen-2.5-14B-GRPO-safety-v1: {'sosoai/Hansol-Qwen-2.5-14B-GRPO-safety-v1-mlx'}\n",
      "raisinghanijayant/my-finetuned-her2-check-2: {'raisinghanijayant/my-finetuned-her2-check-2-Q2_K-GGUF', 'raisinghanijayant/my-finetuned-her2-check-2-Q4_0-GGUF', 'raisinghanijayant/my-finetuned-her2-check-2-V1.0-Q4_0-GGUF'}\n",
      "samoline/2ad4c1d4-bafc-4ab9-a34c-b2ea62abeb15: {'lesso02/09263244-2340-4d99-9ff7-3975d61fd366', 'lesso11/3af885d5-8fa1-4bc3-98e0-de70dadf1441'}\n",
      "BlcaCola/YI-AI-4B-Chinese-IT-V1: {'BlcaCola/YI-AI-4B-Chinese-IT-V1-GGUF'}\n",
      "azizp128/javanese-sentiment-analysis-indobert: {'johannawawi/results'}\n",
      "barc0/cot-400k-barc-llama3.1-8b-ins-fft-transduction_lr1e-5_epoch3: {'barc0/cot-trainset-ft-transduction-v2-lora-train'}\n",
      "samoline/77596f06-1c95-4e04-9067-d9078ee3a804: {'lesso10/7802d367-3022-487c-a295-ee2c594ff19a', 'lesso15/d019a130-9617-4b25-9590-56283473a612'}\n",
      "Hastagaras/run-22-8b-test: {'Hastagaras/run-22-8b-test-Q5_K_M-GGUF'}\n",
      "tarundachepally/EGL_GRANITE_8b_testmodel1: {'tarundachepally/EGL_GRANITE_8b_testmodel1-Q5_K_M-GGUF'}\n",
      "samoline/36f2892b-8311-4471-a38a-9a69406d27bc: {'lesso12/8f930a4d-041b-4f67-9c29-885fb836cc9d'}\n",
      "samoline/6c3dc756-8269-4fcb-a031-aa24ed81a68b: {'lesso13/3bdbbd7d-bbcd-408d-8960-93a96599a2e0', 'lesso10/e3d73612-07f6-41fa-8b66-ff6a0c8504f6'}\n",
      "samoline/1359a1f2-a078-4f60-8e5f-5768b9a21c50: {'lesso13/c1d1d961-5623-4d28-819f-f3f067e729a6', 'lesso10/0216b238-eaa0-450e-ba97-f7193ba7d71a'}\n",
      "MrRobotoAI/R4: {'MrRobotoAI/R7'}\n",
      "deca-ai/2-pro-medical: {'deca-ai/2-pro-beta'}\n",
      "barc0/cot-transduction-arc-heavy: {'barc0/cot-trainset-ft-transduction-v4-lora-train', 'barc0/cot-trainset-ft-transduction-v3-lora-train'}\n",
      "kromcomp/L3.1-Punicav1-12B: {'kromcomp/L3.1-Flagrantv1-12B'}\n",
      "samoline/022c7873-0220-4b23-b4f0-63e468442ecb: {'lesso01/8f70f57d-c296-4302-adcc-078d0cb06409', 'lesso18/2f0bac17-00dc-4e03-a8d4-6a94fc3b518a'}\n",
      "rayonlabs/9d60cc53-cac5-42bd-b5c8-dfb6d6bfced5: {'lesso13/132d4047-9660-4482-bb0e-71f7ca6a2bff', 'lesso14/3f878849-8cb5-4648-85e4-de52dc2374bd'}\n",
      "llm-jp/llm-jp-3-7.2b-instruct2: {'swdq/llm-jp-21B'}\n",
      "samoline/907061aa-f4c4-4026-ba1f-66527f4c18d1: {'lesso02/f0e81a68-499e-43b1-8e54-c38a4e3e4559', 'lesso12/07f0e69f-5af8-4bd5-8507-89c7bcf1eee4'}\n",
      "rayonlabs/83847950-33bc-4506-ba82-48653a06540a: {'lesso13/3b558ad3-fa2e-4574-a683-17fa037fe0c5', 'lesso14/9f63d136-bc42-4985-8cd0-93361d098678'}\n",
      "TareksLab/Protobase-X-LLaMa-70B: {'TareksLab/Unnamed-Experimental-Model-70B'}\n",
      "\n",
      " 0514: 13 clusters\n",
      "collabora/whisper-tiny-hindi: {'Rank001/whisper-tiny-hindi-ONNX'}\n",
      "AntonV/mamba2-130m-hf: {'raphael-lesmana/mamba2_latin'}\n",
      "TNSA-AI/N-Gen-2: {'TNSA-AI/NGen2Base'}\n",
      "pi-de-pie/finetuned-nllb: {'pi-de-pie/finetuned_nllb'}\n",
      "castorini/afriteva_small: {'ahmadmwali/afriteva-hausa-norm-lora-t5-afriteva_small-lora'}\n",
      "classla/xlm-r-parla: {'classla/Multilingual-CAP-Topic-Model-for-Parliamentary-Speeches-ParlaCAP'}\n",
      "macpaw-research/o10r_parameter_parcing_v3_0: {'macpaw-research/o10r_parameter_parcing_v3_0_mlx_q'}\n",
      "parthiv11/stt_hi_conformer_ctc_large_v2: {'WhissleAI/speech-tagger_pa_ctc_meta', 'WhissleAI/speech-tagger_indo-aryan_ctc_meta', 'WhissleAI/speech-tagger_mr_ctc_meta', 'WhissleAI/speech-tagger_hi_ctc_meta'}\n",
      "Juicesyo/Saffi-beta: {'Juicesyo/Saffi-beta-Q4_K_M-GGUF'}\n",
      "macpaw-research/tst_16bit: {'macpaw-research/tst_16bit-mlx-q', 'macpaw-research/tst_16bit-mlx'}\n",
      "trl-internal-testing/tiny-GPTNeoXForCausalLM: {'LTS-VVE/tommorri-reliachat'}\n",
      "unsloth/Mistral-Large-Instruct-2407-bnb-4bit: {'hcstubbe/Mistral-Large-Instruct-2407-bnb-4bit'}\n",
      "SuperbEmphasis/qwen4-test: {'SuperbEmphasis/qwen4-test-Q8_0-GGUF'}\n",
      "\n",
      " 0521: 32 clusters\n",
      "jnises/gemma-3-llmog: {'jnises/gemma-3-1b-llmog-Q8_0-GGUF'}\n",
      "NewEden/Archaeo-32B-KTO: {'Delta-Vector/Axo-Merge-Archaeo-V2-Lora-Q5_0-GGUF', 'Delta-Vector/Axo-Merge-Archaeo-V2-Lora-Q4_K_M-GGUF'}\n",
      "Helsinki-NLP/opus-mt-en-ig: {'pt4c/marian-finetuned-kde4-en-to-ig', 'pt4c/marian-finetuned-kde4-en-to-bbj'}\n",
      "PhucMap/BARTphoVi-VSL: {'ntkhoi/bart-vi-news-summarization'}\n",
      "maddi99/uiu_bn_new_8: {'maddi99/uiu_bn_new_9'}\n",
      "NewEden/V4-lora: {'NewEden/kalomaze-nemo-subseq-perseq-e1.5-V4-lora-merged'}\n",
      "timm/vit_small_patch16_384.augreg_in21k_ft_in1k: {'aiwithoutborders-xyz/OpenSight-CommunityForensics-Deepfake-ViT'}\n",
      "LA1512/pretrained_LoRa_merged: {'LA1512/checkpoint-finetuning'}\n",
      "gabberdotdev/orpheus-finetune-fp16: {'gabberdotdev/orpheus-lora-zeer'}\n",
      "kamelcharaf/Mistral-Small-24B-Instruct-2501-quantized-4bit: {'kamelcharaf/SFT-mistral-24B-quant-mrd3'}\n",
      "kamelcharaf/Qwen2.5-32B-Instruct-quantized-4bit: {'kamelcharaf/SFT-qwen2.5-Llama-32B-quant-mrd3'}\n",
      "wareocraftworld/Alanta-V1: {'mradermacher/Alanta-V1-GGUF'}\n",
      "NewEden/GLM-New-V3: {'Delta-Vector/GLM-New-V3-Q5_0-GGUF'}\n",
      "NewEden/Rae-Instruct: {'Delta-Vector/Rae-Instruct-Q6_K-GGUF'}\n",
      "kamelcharaf/gemma-2-27b-it-quantized-4bit: {'kamelcharaf/SFT-gemma-Gemma-27B-quant-mrd3'}\n",
      "MendelAI/DocResolve-Qwen2.5-32B-v0.0.1: {'thomasr-mendel/debug_qwen32'}\n",
      "Nitral-AI/Nemotron-15b-Thinker-v0.2: {'Nitrals-Quants/Nemotron-15b-Thinker-v0.2-Q4_K_M-GGUF', 'Nitrals-Quants/Nemotron-15b-Thinker-v0.2-Q5_K_M-GGUF', 'Nitrals-Quants/Nemotron-15b-Thinker-v0.2-Q8_0-GGUF'}\n",
      "NewEden/KTO-rae-adaptor: {'NewEden/rae-12b-kto-with-lora-merged'}\n",
      "KatSoEpic/Discord-Mistral-V2-Completions-Merged: {'KatSoEpic/Discord-Mistral-7B-V2.5-GGUF'}\n",
      "Edens-Gate/Hamanasu-4B-Adventure-CKPTS: {'Delta-Vector/adv-4b-v2-e6'}\n",
      "kromcomp/L3.1-Eleusis.Concv1-12B: {'kromcomp/L3.1-Promachosv6-12B'}\n",
      "kromcomp/L3.1-Nuancev2-12B: {'kromcomp/L3.1-Promachosv4-12B'}\n",
      "kamelcharaf/Qwen2.5-14B-Instruct-quantized-4bit: {'kamelcharaf/GRPO-qwen2.5-14B-quant-qwen2.5-14B-quant-mrd3-s2-sum', 'kamelcharaf/SFT-qwen2.5-14B-quant-mrd3', 'kamelcharaf/GRPO-SFT-qwen2.5-14B-quant-qwen2.5-14B-quant-mrd3-s2-sum'}\n",
      "kamelcharaf/Llama-3.1-8B-Instruct-quantized-4bit: {'kamelcharaf/SFT-meta-Llama-3.1-8B-quant-mrd3'}\n",
      "kamelcharaf/Phi-3-medium-128k-instruct-quantized-4bit: {'kamelcharaf/SFT-phi-3-14B-quant-mrd3', 'kamelcharaf/GRPO-phi-3-14B-quant-phi-3-14B-quant-mrd3-s3-sum', 'kamelcharaf/GRPO-SFT-phi-3-14B-quant-phi-3-14B-quant-mrd3-s3-sum'}\n",
      "sagawa/ReactionT5v2-forward: {'Bio2Q/best_model_EC_v5_2_2way_EC_linear_batch128_epoch200'}\n",
      "Vale55/gemma4b: {'Vale55/gemma4b-Q8_0-GGUF'}\n",
      "lst-nectec/HoogBERTa-NER-lst20: {'NatthasitW/Hoogberta-NER-ss'}\n",
      "kromcomp/L3.1-Instruct-12B: {'kromcomp/L3.1-Promachosv6-12B', 'kromcomp/L3.1-Smth.Concv6-12B', 'kromcomp/L3.1-Smth.Sub-12B', 'kromcomp/L3.1-Smth.Concv4-12B', 'kromcomp/L3.1-Smth.Concv5-12B', 'kromcomp/L3.1-Promachosv4-12B', 'kromcomp/L3.1-Promachosv5-12B'}\n",
      "raman07/pixart-alpha-256x256: {'raman07/LR_1e-06', 'raman07/LR_1e-05', 'raman07/LR_0.0001'}\n",
      "kromcomp/L3.1-Nuancev3-12B: {'kromcomp/L3.1-Promachosv6-12B', 'kromcomp/L3.1-Promachosv5-12B'}\n",
      "ClassCat/roberta-small-basque: {'IParraMartin/XLM-EusBERTa-sentiment-classification', 'IParraMartin/XLM-EusBERTa-topic-classification'}\n",
      "\n",
      " 0528: 23 clusters\n",
      "timm/efficientvit_b0.r224_in1k: {'emiliaviq/efficientvit-ena24'}\n",
      "Casual-Autopsy/vntl-qlora: {'Casual-Autopsy/Llama-3-VNTL-Yollow-8B-Fixed', 'Casual-Autopsy/Llama-3-VNTL-Yollow-8B-v2-TEST001-Q6_K-GGUF'}\n",
      "MrRobotoAI/Loki-v4.1-8b-EROTICA-128K: {'MrRobotoAI/Loki-v4.1-8b-EROTICA-128K-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/MrRoboto-ROMANCE-v2-8b-128K: {'MrRobotoAI/MrRoboto-ROMANCE-v2-8b-128K-Q4_K_M-GGUF'}\n",
      "Yuichi1218/Lafaek-8B-instruct-05160958: {'Yuichi1218/Lafaek-8B-MSCS-instruct-05191040'}\n",
      "open-neo/OdysseyXL-Origin: {'open-neo/OdysseyXL-V1', 'open-neo/OdysseyXL-V2', 'open-neo/OdysseyXL-V2.5'}\n",
      "TriAiExperiments/L3-8B-Stheno-v3.1: {'MrRobotoAI/L3-8B-Stheno-v3.1-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/Freyja-v6.5-8b-GENERAL-128K: {'MrRobotoAI/Freyja-v6.5-8b-GENERAL-128K-Q4_K_M-GGUF'}\n",
      "Phase-AI/RoBERTo: {'Phase-AI/RoBERTo-physics-v1-finetuned'}\n",
      "funasr/Paraformer-large: {'Raynhu/ELE-AI-Paraformer-Large-SFT'}\n",
      "MrRobotoAI/MrRoboto-HORNY-8b-64k: {'MrRobotoAI/MrRoboto-HORNY-8b-64k-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/MrRoboto-HORNY-v1.5-8b-64k: {'MrRobotoAI/MrRoboto-HORNY-v1.5-8b-64k-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/ResplendentAI-ChattyMix-8b-128k: {'MrRobotoAI/ResplendentAI-ChattyMix-8b-128k-Q4_K_M-GGUF'}\n",
      "alperenyildiz/SmolGRPO_vuln: {'alperenyildiz/SmolGRPO_vuln-Q4_K_M-GGUF'}\n",
      "Yuichi1218/20250408Tetun-unsloth-llama3.1-instruct-bnb4bit: {'Yuichi1218/Lafaek-8B-instruct-04182118'}\n",
      "alperenyildiz/SmolGRPO-135M: {'alperenyildiz/SmolGRPO-135M-Q4_K_M-GGUF'}\n",
      "MrRobotoAI/Thor-v2.5-8b-FANTASY-FICTION-128K: {'MrRobotoAI/Thor-v2.5-8b-FANTASY-FICTION-128K-Q4_K_M-GGUF'}\n",
      "greenwich157/granite-3.3-8b-instruct-telcollm-c: {'greenwich157/granite-3.3-8b-instruct-telcollm-c-Q8_0-GGUF'}\n",
      "speechbrain/lang-id-commonlanguage_ecapa: {'panchajanya-ai/vaani'}\n",
      "greenwich157/granite-3.3-8b-instruct-telcollm-a: {'greenwich157/granite-3.3-8b-instruct-telcollm-a-Q8_0-GGUF'}\n",
      "macpaw-research/o10r_parameter_parcing_v2_0_mlx_float16: {'macpaw-research/o10r_parameter_parcing_v2_0_mlx_int8'}\n",
      "iamTangsang/MarianMT-Nepali-to-English: {'iamTangsang/MarianMT-Nepali-to-English-Synthetic-Pretrain-Continued', 'iamTangsang/208k-MarianMT-Nepali-to-English'}\n",
      "hubble658/obss-merged-qwen-new: {'hubble658/obss-qwen-7b-1.5ep'}\n",
      "\n",
      " 0604: 26 clusters\n",
      "ymoslem/qwen-audio-en-zh-ft-decoder-32-24layers: {'ymoslem/qwen-audio-en-zh-ft-decoder-32-24layers-4bs-1e5lr-cosine-3epoch-acl6060kd-new'}\n",
      "greenwich157/qwen3-4b-telcollm-d: {'greenwich157/qwen3-4b-telcollm-d-Q8_0-GGUF'}\n",
      "AhmedZaky1/arabic-bert-nli-matryoshka: {'AhmedZaky1/arabic-bert-sts-matryoshka'}\n",
      "green19d25y/Qwen2-32m-hf: {'green19d25y/Qwen2-32m-hf-ftv1'}\n",
      "greenwich157/nemotron-nano-8b-telcollm-h: {'greenwich157/nemotron-nano-8b-telcollm-h-Q8_0-GGUF'}\n",
      "greenwich157/qwen3-4b-telcollm-b: {'greenwich157/qwen3-4b-telcollm-b-Q8_0-GGUF'}\n",
      "openbmb/Eurus-7b-sft: {'just1nseo/eurus-dpop-qlora-uf-5e-7-real', 'just1nseo/eurus-dpo-qlora-uf-5e-6', 'just1nseo/eurus-dpop-qlora-uf-ours-5e-6', 'just1nseo/eurus-dpo-qlora-uffull-5e-6', 'just1nseo/eurus-7b-cost-UI-UC-5e-7', 'just1nseo/eurus-7b-cost-UF-5e-7', 'just1nseo/eurus-dpop-qlora-uffull-5e-6', 'just1nseo/eurus-dpop-qlora-uf-ours-uffull-5e-6', 'just1nseo/eurus-dpop-qlora-uf-ours-5e-7', 'just1nseo/eurus-dpop-qlora-uf-ours-uffull-5e-7', 'just1nseo/eurus-dpo-qlora-uf-ours-5e-6', 'just1nseo/eurus-dpop-qlora-uf-5e-7', 'just1nseo/eurus-dpo-qlora-uf-ours-5e-7', 'just1nseo/eurus-dpo-qlora-uf-ours-uffull-5e-6', 'just1nseo/eurus-dpop-qlora-uf-5e-6', 'just1nseo/eurus-7b-cost-UI-5e-7', 'just1nseo/eurus-7b-cost-UC-5e-7', 'just1nseo/eurus-dpo-qlora-uf-ours-uffull-5e-7'}\n",
      "gabberdotdev/orpheus-finetune-fp16-es: {'gabberdotdev/orpheus-lora-snaps'}\n",
      "parthiv11/stt_hi_conformer_ctc_large_v2: {'WhissleAI/Meta_STT_INDO-ARYAN_AI4Bharat', 'WhissleAI/Meta_STT_PA_AI4Bharat', 'WhissleAI/Meta_STT_MT_AI4Bharat', 'WhissleAI/Meta_STT_HI_AI4Bharat'}\n",
      "Hastagaras/q4b-run-29-test: {'Hastagaras/q4b-run-29-test-Q6_K-GGUF'}\n",
      "nil6753/gemma3_fine_250502: {'nil6753/gemma3_fine_250502-Q8_0-GGUF'}\n",
      "soob3123/GrayLine-Qwen3-14B-Planner-V1a: {'soob3123/GrayLine-Qwen3-14B-Planner-V1a-Q4_K_M-GGUF'}\n",
      "green19d25y/gpt2-23m-hf: {'green19d25y/gpt2-23m-hf-ftv1'}\n",
      "YWChang/Llama-3.2-finetuned: {'YWChang/Llama-3.2-finetuned-Q4_K_M-GGUF'}\n",
      "soob3123/GrayLine-Qwen3-14B-Planner-V1b: {'soob3123/GrayLine-Qwen3-14B-Planner-V1b-Q4_K_M-GGUF'}\n",
      "xzxiong/my_model: {'xzxiong/my_model-F16-GGUF'}\n",
      "ricostaedeli/Meta-Llama-3.1-1B-Instruct_SFT_2: {'ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO-lora'}\n",
      "ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2: {'ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1', 'ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO', 'ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1-lora'}\n",
      "Abdul-Ib/multilingual-en-ar-2024: {'KARAKOZA/bilingual-embedder-en-ar'}\n",
      "Hsianchengfun/1B-200epoch_20: {'while0628/1B-200epoch_20-lora-15000'}\n",
      "tori29umai/animagine-xl-3.1_VRoidstyle: {'Visual-Bank/CN_ControlBody_v3'}\n",
      "soob3123/GrayLine-Qwen3-14B-Planner-V1c: {'soob3123/GrayLine-Qwen3-14B-Planner-V1c-Q4_K_M-GGUF'}\n",
      "sujalrajpoot/Ira_EQ-1B: {'sujalrajpoot/Ira_EQ-1B-Q5_K_M-GGUF', 'sujalrajpoot/Ira_EQ-1B-Q6_K-GGUF', 'sujalrajpoot/Ira_EQ-1B-Q4_K_M-GGUF', 'sujalrajpoot/Ira_EQ-1B-Q8_0-GGUF'}\n",
      "chs20/fare2-clip: {'RCLIP/CLIP-ViT-L-rho50-k1-FARE2'}\n",
      "FF2416/sft_scp_epoch1: {'hsicat/DPO-scp'}\n",
      "greenwich157/nemotron-nano-8b-telcollm: {'greenwich157/nemotron-nano-8b-telcollm-Q8_0-GGUF'}\n",
      "\n",
      " 0611: 20 clusters\n",
      "saransh03sharma/sft_model_state_dict-full: {'saransh03sharma/sft_model_RESTA'}\n",
      "saransh03sharma/dare_merged_model_state_dict-full: {'saransh03sharma/dare_full_RESTA'}\n",
      "collinear-ai/de_duplicate: {'collinear-ai/de_duplicate_fine_tuned_hard_correct'}\n",
      "IbrahimSalah/Orphus_10k: {'IbrahimSalah/Orphus_100'}\n",
      "publication-charaf/OA_Qwen3-0.6B-Base_lr-1e-05_e-1_s-0: {'publication-charaf/MIX_OA_Qwen3-0.6B-Base_lr-1e-05_e-1_s-0_lr-5e-06_e-1_s-0'}\n",
      "saransh03sharma/sft_model_state_dict-peft: {'saransh03sharma/peft_model_RESTA'}\n",
      "qayemmehdi/mnlp_sft: {'qayemmehdi/dpo_qwen', 'qayemmehdi/mnlp_dpo_test', 'qayemmehdi/new_dpo_2', 'qayemmehdi/mnlp_dpo4'}\n",
      "PsycheFoundation/consilience-40b-7Y9v38s5: {'DevQuasar/PsycheFoundation.consilience-40b-7Y9v38s5-GGUF'}\n",
      "golyuval/SciGuru-zero: {'golyuval/SciGuru-alpha-1'}\n",
      "minpeter/pretrained-tiny-ko: {'minpeter/ko-tiny-exp'}\n",
      "Wan-AI/Wan2.1-VACE-14B: {'QuantStack/Wan2.1-VACE-14B-GGUF'}\n",
      "anvorja/xlm-roberta-medico-vocabulario-extendido_v2: {'anvorja/xlm-roberta-large-clinical-ner-vocabulario-extendido-v2-alingRau-sp'}\n",
      "NEWWWWWbie/cybertron_merge_02: {'NEWWWWWbie/cybertron_merge_02-Q8_0-GGUF', 'NEWWWWWbie/cybertron_merge_02-Q4_K_M-GGUF'}\n",
      "publication-charaf/OA_Qwen3-0.6B-Base_lr-1e-05_e-3_s-0: {'publication-charaf/MIX_OA_Qwen3-0.6B-Base_lr-1e-05_e-3_s-0_lr-1e-05_e-1_s-0'}\n",
      "saransh03sharma/sft-unsafe-model-full: {'saransh03sharma/peft_model_RESTA', 'saransh03sharma/dare_full_RESTA', 'saransh03sharma/dare_peft_RESTA', 'saransh03sharma/sft_model_RESTA'}\n",
      "gg-ai/xml-bert-1312: {'Tsegayesemere/Tsegay-Tigrigna-tokenizer'}\n",
      "saransh03sharma/dare_merged_model_state_dict-peft: {'saransh03sharma/dare_peft_RESTA'}\n",
      "anvorja/xlm-roberta-medico-vocabulario-extendido: {'anvorja/xlm-roberta-large-clinical-ner-vocabulario-extendido-alingRau-sp'}\n",
      "NEWWWWWbie/cybertron_merge_01: {'NEWWWWWbie/cybertron_merge_01-Q8_0-GGUF', 'NEWWWWWbie/cybertron_merge_01-Q4_K_M-GGUF'}\n",
      "lianghsun/F1-24B-Instruct-Cybersecurity: {'lianghsun/F1-24B-Instruct-Cybersecurity-Q4_K_M-GGUF'}\n",
      "\n",
      " 0618: 38 clusters\n",
      "Chi666/deepseek-ai_DeepSeek-R1-Distill-Llama-70B_finetune_20250209: {'yuvraj-yadav/math'}\n",
      "DeepPavlov/distilrubert-small-cased-conversational: {'NeuroSpaceX/ruSpamNS_big'}\n",
      "dumitrescustefan/bert-base-romanian-uncased-v1: {'Sabb2122/second_robert_results'}\n",
      "FlameF0X/Snowflake: {'FlameF0X/Muffin-3.0-1B'}\n",
      "line-corporation/japanese-large-lm-1.7b-instruction-sft: {'aipib/line-slerp2'}\n",
      "IlyaGusev/rubertconv_toxic_clf: {'MesserMMP/rubertconv_toxic_clf_finetuned'}\n",
      "newmindai/TurkEmbed4STS: {'ozayezerceli/TurkEmded4Retrievalv2'}\n",
      "sujalrajpoot/JARVIS-7B: {'sujalrajpoot/JARVIS-7B-GGUF'}\n",
      "ai-forever/Real-ESRGAN: {'danhtran2mind/finetuning-Real-ESRGAN-anime'}\n",
      "ai21labs/AI21-Jamba-Large-1.6: {'hf-100/Jamba-1.6-large-Spellbound-StoryWriter-398B94A-instruct-0.1-chkpt-78-adapter', 'hf-100/Jamba-1.6-large-Spellbound-StoryWriter-398B94A-instruct-0.1-chkpt-234-adapter', 'hf-100/Jamba-1.6-large-Spellbound-StoryWriter-398B94A-instruct-0.1-chkpt-156-adapter', 'hf-100/Jamba-1.6-large-Spellbound-StoryWriter-398B94A-instruct-0.1-chkpt-312-adapter'}\n",
      "stablediffusionapi/kawaii-realistic-anime-mi: {'hsuwill000/kawaii-realistic-anime-mi-openvino'}\n",
      "oshizo/qa-refine-japanese-gpt-1b: {'aipib/jp-gpt-1b-dareties2'}\n",
      "alfredplpl/suzume-poc: {'aipib/suzume-linear1'}\n",
      "harissonduraes/fine-tuning: {'harissonduraes/fine-tuning-F16-GGUF'}\n",
      "AnReu/math_pretrained_bert: {'aieng-lab/math_pretrained_bert-mamut'}\n",
      "AlicanKiraz0/QwQ-32B-Preview-SenecaLLMv1.2: {'AlicanKiraz0/QwQ-32B-Preview-SenecaLLMv1.2-Q8_0-GGUF', 'AlicanKiraz0/QwQ-32B-Preview-SenecaLLMv1.2-Q4_K_M-GGUF'}\n",
      "Theros/L3-ColdBrew-Astrid-Ascension: {'Theros/L3-ColdBrew-Astrid-Ascension-Q5_K_M-GGUF'}\n",
      "7h3-R3v3n4n7/pentest-agent-merged-4bit: {'7h3-R3v3n4n7/pentest-agent-q4_k_m-gguf', '7h3-R3v3n4n7/pentest-agent-8bit-Q8_0-gguf', '7h3-R3v3n4n7/pentest-agent-16bit-gguf'}\n",
      "Rif010/llama3.2-3b-it-ecommerce-chatbot-merged: {'Rif010/llama3.2-3b-it-ecommerce-chatbot-merged-Q8_0-GGUF'}\n",
      "marissaliora/merged-llama2: {'marissaliora/llama2_merged'}\n",
      "tencent/HunyuanVideo-Avatar: {'lym00/HunyuanVideo-Avatar-GGUF'}\n",
      "Algorine/whisper-large-v3-turbo-twi_v2: {'azunre/lora_whisper_large_twi_multi_adapter'}\n",
      "google/pegasus-pubmed: {'Hendrik-a/models'}\n",
      "ce-lery/dolly-japanese-gpt-1b-clone: {'aipib/jp-gpt-1b-dareties'}\n",
      "ncauchi1/image_pointing_merged_temp: {'ncauchi1/pointing_demo_5k_adapter_2'}\n",
      "rinna/japanese-wav2vec2-base: {'SiRoZaRuPa/wav2vec2-kanji-base-unigram-0914', 'SiRoZaRuPa/wav2vec2-base-kanji-unigram-RS-s-1120', 'SiRoZaRuPa/wav2vec2-kanji-base-char-0916'}\n",
      "gdhe17/Self-Forcing: {'lym00/Wan2.1-T2V-1.3B-Self-Forcing-VACE'}\n",
      "qqfang97/r1-q1: {'qqfang97/r1-q3', 'qqfang97/r1-q3-Q4_K_M-GGUF', 'qqfang97/r1-q2'}\n",
      "minishlab/potion-base-32M: {'Jarbas/ovos-model2vec-intents-potion-32M'}\n",
      "mideind/IceBERT-large: {'asibasi25/payment-codes-classifier'}\n",
      "alpindale/gemma-2b-it: {'aipib/suzume-linear1'}\n",
      "Fastweb/FastwebMIIA-7B: {'DevQuasar/Fastweb.FastwebMIIA-7B-GGUF'}\n",
      "bunnycore/Llama-3.2-3B-RP-LORA: {'Theros/llama3.2-CharReflectBase-3B'}\n",
      "rinna/japanese-hubert-large: {'SiRoZaRuPa/hubert-japanese-large-noise-0427'}\n",
      "sujalrajpoot/TrueSyncGenZ-7B: {'sujalrajpoot/TrueSyncGenZ-7B-GGUF'}\n",
      "KatyTheCutie/R1-1.5B: {'KatyTheCutie/R1-1.5B-Q8_0-GGUF'}\n",
      "mesolitica/llama-7b-hf-32768-fpf: {'Supa-AI/llama-7b-hf-32768-fpf-gguf'}\n",
      "Helsinki-NLP/opus-mt-en-gmq: {'Confused404/eng-gmq-finetuned-no'}\n",
      "\n",
      " 0625: 8 clusters\n",
      "PhanithLIM/whisper-tiny-aug-29dec: {'PhanithLIM/whisper-tiny-aug-14-03-25'}\n",
      "PhanithLIM/whisper-khmer-base-v4: {'PhanithLIM/whisper-khmer-base-v5'}\n",
      "Kwoya/Spyra-v0.1: {'Kwokou/Spyra-v0.1-Q8_0-GGUF'}\n",
      "kpsss34/SANA600.fp16_illustrious_SFW_V1: {'kpsss34/Sana600m_lora_y0r_4ger_demo1'}\n",
      "muhtasham/orpheus-tj: {'muhtasham/orpheus-tj-mlx-fp16'}\n",
      "JFernandoGRE/llama31_8b_augmenteddemocracy_sft_questions_50_critsupport: {'JFernandoGRE/llama31_8b_augmenteddemocracy_grpodpo_questions_50_critsupport', 'JFernandoGRE/llama31_8b_augmenteddemocracy_grpo_questions_50_critsupport'}\n",
      "bughug0x/py311-pylingual-v1-segmenter-mlm: {'bughug0x/py311-pylingual-v1-segmenter-segmenter'}\n",
      "NLP-EXP/QE3: {'afnan89/temp_emo_classi', 'afnan89/emotion_fold_1', 'afnan89/temp_stl_classi'}\n",
      "\n",
      " 0702: 29 clusters\n",
      "skyfury/CTMEDBERT-cl-step_45000: {'skyfury/CTMEDBERT_CLS_Encoder'}\n",
      "skyfury/CTMEDGTE-cl5-step_22000: {'skyfury/CTMEDGTE5_encoder'}\n",
      "lightx2v/Wan2.1-T2V-14B-StepDistill-CfgDistill: {'lym00/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF'}\n",
      "skyfury/CTMEDE5-cl1-step_8000: {'skyfury/CTMEDE5_CLS_Encoder1'}\n",
      "NewEden/glm-merge-0-2: {'Delta-Vector/glm-merge-0-2-Q5_K_M-GGUF'}\n",
      "skyfury/CTMEDGTE-cl1-step_18000: {'skyfury/CTMEDGTE_CLS_Encoder1'}\n",
      "DeepPavlov/distilrubert-base-cased-conversational: {'undefinedhorizons/ner-llm-distilbert'}\n",
      "skyfury/CTMEDGTE-cl2-step_12000: {'skyfury/CTMEDGTE_CLS_Encoder2', 'skyfury/CTMEDGTE2_encoder'}\n",
      "NewEden/Qwen-3-MSA-KTO: {'Delta-Vector/Qwen-3-MSA-KTO-Q8_0-GGUF'}\n",
      "NeuML/glove-6B: {'NeuML/glove-6B-quantized'}\n",
      "SaiCharan0017/speecht5_finetuned_sai_charan_v3: {'SaiCharan0017/speecht5_finetuned_sai_charan_v4'}\n",
      "keras-io/CycleGAN: {'Borcherding/CycleGAN_Depth2RobotsV2_Blend'}\n",
      "touseef2002/llama-3.2-3b-it-Labels-ChatBot-GGUF: {'touseef2002/llama-3.2-3b-it-Labels-ChatBot-GGUF-Q4_K_M-GGUF'}\n",
      "ymoslem/qwen-audio-en-de-ft-decoder-32-24layers-iterative-pruning-chrf: {'ymoslem/qwen-audio-en-de-ft-decoder-32-24layers-iter-pruning-chrf-4bs-1e5lr-cosine-3epoch-acl-kd1-50'}\n",
      "Salesforce/xgen-small-9B-base-r: {'Nitral-AI/SekhmetX-9B-Reasoner-v0.420'}\n",
      "izzymiller95/caret-beta-1: {'izzymiller95/caret-1-dpo-relabeled', 'izzymiller95/caret-1-dpo-raw'}\n",
      "skyfury/CTMEDGTE-cl6-step_20000: {'skyfury/CTMEDGTE6_encoder'}\n",
      "BAAI/EVA-CLIP-8B: {'exdysa/EVA-CLIP-8B-SFT'}\n",
      "skyfury/CTMEDGTE-cl7-step_6000: {'skyfury/CTMEDGTE7_encoder'}\n",
      "UUFO-Aigis/Planck-OpenLAiNN-10M: {'exdysa/Pico-OpenLAiNN-10M-GGUF'}\n",
      "skyfury/CTMEDBERT-cl2-step_45000: {'skyfury/CTMEDBERT_CLS_Encoder2'}\n",
      "MIT/ast-finetuned-audioset-12-12-0.447: {'mahmoudmamdouh13/ast-beta-finetuned-en-alphabets'}\n",
      "skyfury/CTMEDBERT-cl4-step_16000: {'skyfury/CTMEDBERT_CLS_Encoder4'}\n",
      "somelier/gemmatron-zero-27b: {'somelier/gemmatron-zero-27b-Q5_K_S-GGUF'}\n",
      "skyfury/CTMEDBERT-cl3-step_18000: {'skyfury/CTMEDBERT_CLS_Encoder3'}\n",
      "BAAI/EVA-CLIP-18B: {'exdysa/EVA-CLIP-18B-SFT'}\n",
      "ByteWave/gpt2-turkish-uncased: {'metncelik/tr-lyrics-generator-gpt2-uncased'}\n",
      "OUGeebar/bert-base-multilingual-uncased-b02: {'jjzzc/bert-base-multilingual-uncased-b02-ONNX'}\n",
      "tooning/spicy-tooning-plus-16bit: {'tooning/spicy-tooning-plus-16bit-20250227-gguf', 'tooning/tooning-plus-llm-model-16bit-20250301154509', 'tooning/spicy-tooning-plus-8bit-20250227-gguf'}\n",
      "\n",
      "Average: 25.69 \n",
      "Average Size: 2.45\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "cluster_files = { \n",
    "    os.path.basename(f).replace(\"model_clusters_\", \"\").replace(\".pkl\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_clusters/model_clusters_*.pkl\"))\n",
    "}\n",
    "\n",
    "weekly_full_deleted_clusters = {}\n",
    "\n",
    "for del_path in deleted_files:\n",
    "    del_date = os.path.basename(del_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        del_dt = datetime.strptime(del_date, \"%m%d\")\n",
    "        cluster_dt = del_dt - timedelta(days=7)\n",
    "        cluster_date = cluster_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse Date failed: {del_date},skipping.Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    cluster_path = cluster_files.get(cluster_date)\n",
    "    if not cluster_path:\n",
    "        print(f\"File cannot be found: model_clusters_{cluster_date}.pkl,skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(del_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    with open(cluster_path, \"rb\") as f:\n",
    "        cluster_data = pickle.load(f)\n",
    "\n",
    "    full_deleted_clusters = []\n",
    "    for cluster_id, models in cluster_data.items():\n",
    "        if all(m in deleted_models for m in models):\n",
    "            full_deleted_clusters.append((cluster_id, models))\n",
    "\n",
    "    if full_deleted_clusters:\n",
    "        weekly_full_deleted_clusters[del_date] = full_deleted_clusters\n",
    "\n",
    "for del_date, clusters in weekly_full_deleted_clusters.items():\n",
    "    print(f\"\\n {del_date}: {len(clusters)} clusters\")\n",
    "    for cluster_id, models in clusters:\n",
    "        print(f\"{cluster_id}: {models}\")\n",
    "\n",
    "total_clusters = sum(len(clusters) for clusters in weekly_full_deleted_clusters.values())\n",
    "weeks = len(weekly_full_deleted_clusters)\n",
    "avg_clusters_per_week = total_clusters / weeks if weeks else 0\n",
    "\n",
    "all_cluster_sizes = [len(models) for clusters in weekly_full_deleted_clusters.values() for _, models in clusters]\n",
    "avg_cluster_size = sum(all_cluster_sizes) / len(all_cluster_sizes) if all_cluster_sizes else 0\n",
    "\n",
    "print(f\"\\nAverage: {round(avg_clusters_per_week, 2)} \")\n",
    "print(f\"Average Size: {round(avg_cluster_size, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models form new chains and add to pre-existing chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weekly Relational Model Distribution ===\n",
      "0319: Relational=8422, Pre-existing=7675, New Chains=747\n",
      "0326: Relational=8768, Pre-existing=8123, New Chains=645\n",
      "0402: Relational=9703, Pre-existing=9085, New Chains=618\n",
      "0409: Relational=9905, Pre-existing=9298, New Chains=607\n",
      "0416: Relational=7280, Pre-existing=6682, New Chains=598\n",
      "0423: Relational=7986, Pre-existing=7470, New Chains=516\n",
      "0430: Relational=8570, Pre-existing=7534, New Chains=1036\n",
      "0507: Relational=9317, Pre-existing=8846, New Chains=471\n",
      "0514: Relational=7046, Pre-existing=6612, New Chains=434\n",
      "0521: Relational=8149, Pre-existing=7614, New Chains=535\n",
      "0528: Relational=6697, Pre-existing=6381, New Chains=316\n",
      "0604: Relational=6768, Pre-existing=6167, New Chains=601\n",
      "0611: Relational=6032, Pre-existing=5716, New Chains=316\n",
      "0618: Relational=9421, Pre-existing=8781, New Chains=640\n",
      "0625: Relational=1687, Pre-existing=1557, New Chains=130\n",
      "0702: Relational=6578, Pre-existing=6117, New Chains=461\n",
      "\n",
      " === Summary ===\n",
      "\n",
      "Average per week: Pre-existing=7103.62, New Chains=541.94\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_path = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "weekly_new_chain_counts = []\n",
    "weekly_pre_existing_counts = []\n",
    "\n",
    "current = start_date\n",
    "\n",
    "print(\"=== Weekly Relational Model Distribution ===\")\n",
    "\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    \n",
    "    root_file = os.path.join(base_path, f\"added_root_{date_str}.json\")\n",
    "    chain_file = os.path.join(base_path, f\"model_chains/model_chains_{date_str}.txt\")\n",
    "    model_file = os.path.join(base_path, f\"added_models/added_model_{date_str}.csv\")\n",
    "    degree_file = os.path.join(base_path, f\"model_degree_{date_str}.csv\")\n",
    "\n",
    "    try:\n",
    "        with open(root_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            added_roots = set(json.load(f))\n",
    "\n",
    "        all_models = set()\n",
    "        with open(chain_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or \"->\" not in line:\n",
    "                    continue\n",
    "                parts = [p.strip() for p in line.split(\"->\")]\n",
    "                root = parts[0]\n",
    "                if root in added_roots:\n",
    "                    all_models.update(parts)\n",
    "\n",
    "        df_added = pd.read_csv(model_file)\n",
    "        added_model_ids = set(df_added[\"Model ID\"].dropna())\n",
    "        new_models_in_chains = [m for m in all_models if m in added_model_ids]\n",
    "        new_chain_count = len(new_models_in_chains)\n",
    "\n",
    "        if os.path.exists(degree_file):\n",
    "            degree_df = pd.read_csv(degree_file)\n",
    "            filtered_df = degree_df[degree_df[\"Model ID\"].isin(added_model_ids)]\n",
    "            relational_count = len(filtered_df[(filtered_df[\"In-degree\"] > 0) | (filtered_df[\"Out-degree\"] > 0)])\n",
    "        else:\n",
    "            relational_count = 0\n",
    "\n",
    "        pre_existing_count = max(relational_count - new_chain_count, 0)  \n",
    "\n",
    "        weekly_new_chain_counts.append(new_chain_count)\n",
    "        weekly_pre_existing_counts.append(pre_existing_count)\n",
    "\n",
    "        print(f\"{date_str}: Relational={relational_count}, \"\n",
    "              f\"Pre-existing={pre_existing_count}, New Chains={new_chain_count}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Missing file: {e.filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {e}\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if weekly_new_chain_counts:\n",
    "    avg_new = sum(weekly_new_chain_counts) / len(weekly_new_chain_counts)\n",
    "    avg_pre = sum(weekly_pre_existing_counts) / len(weekly_pre_existing_counts)\n",
    "\n",
    "    print(\"\\n === Summary ===\")\n",
    "    print(f\"\\nAverage per week: Pre-existing={avg_pre:.2f}, New Chains={avg_new:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the length of added model chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0319: 508 chains, average length = 2.38\n",
      "0326: 394 chains, average length = 2.15\n",
      "0402: 386 chains, average length = 2.04\n",
      "0409: 411 chains, average length = 2.11\n",
      "0416: 3025 chains, average length = 10.61\n",
      "0423: 382 chains, average length = 2.29\n",
      "0430: 817 chains, average length = 2.55\n",
      "0507: 313 chains, average length = 2.23\n",
      "0514: 265 chains, average length = 2.11\n",
      "0521: 352 chains, average length = 2.04\n",
      "0528: 198 chains, average length = 2.03\n",
      "0604: 502 chains, average length = 2.19\n",
      "0611: 197 chains, average length = 2.03\n",
      "0618: 470 chains, average length = 2.09\n",
      "0625: 80 chains, average length = 2.04\n",
      "0702: 306 chains, average length = 2.03\n",
      "\n",
      "Overall average chain length: 5.16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_path = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "weekly_avg_lengths = []\n",
    "all_lengths = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "\n",
    "    root_file = os.path.join(base_path, f\"added_root_{date_str}.json\")\n",
    "    chain_file = os.path.join(base_path, f\"model_chains/model_chains_{date_str}.txt\")\n",
    "\n",
    "    try:\n",
    "        with open(root_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            added_roots = set(json.load(f))\n",
    "\n",
    "        chain_lengths = []\n",
    "        with open(chain_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or \"->\" not in line:\n",
    "                    continue\n",
    "                parts = [p.strip() for p in line.split(\"->\")]\n",
    "                if parts[0] in added_roots:\n",
    "                    chain_lengths.append(len(parts))\n",
    "\n",
    "        if chain_lengths:\n",
    "            avg_len = sum(chain_lengths) / len(chain_lengths)\n",
    "            weekly_avg_lengths.append(avg_len)\n",
    "            all_lengths.extend(chain_lengths)\n",
    "            print(f\"{date_str}: {len(chain_lengths)} chains, average length = {avg_len:.2f}\")\n",
    "        else:\n",
    "            print(f\"{date_str}: No chains found for added roots.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Missing file: {e.filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {e}\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if all_lengths:\n",
    "    overall_avg = sum(all_lengths) / len(all_lengths)\n",
    "    print(f\"\\nOverall average chain length: {overall_avg:.2f}\")\n",
    "else:\n",
    "    print(\" No chain data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### form added clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0319: 236 clusters saved to ../../data/added_model_clusters_0319.pkl\n",
      " 0326: 239 clusters saved to ../../data/added_model_clusters_0326.pkl\n",
      " 0402: 242 clusters saved to ../../data/added_model_clusters_0402.pkl\n",
      " 0409: 217 clusters saved to ../../data/added_model_clusters_0409.pkl\n",
      " 0416: 195 clusters saved to ../../data/added_model_clusters_0416.pkl\n",
      " 0423: 187 clusters saved to ../../data/added_model_clusters_0423.pkl\n",
      " 0430: 178 clusters saved to ../../data/added_model_clusters_0430.pkl\n",
      " 0507: 159 clusters saved to ../../data/added_model_clusters_0507.pkl\n",
      " 0514: 164 clusters saved to ../../data/added_model_clusters_0514.pkl\n",
      " 0521: 197 clusters saved to ../../data/added_model_clusters_0521.pkl\n",
      " 0528: 117 clusters saved to ../../data/added_model_clusters_0528.pkl\n",
      " 0604: 151 clusters saved to ../../data/added_model_clusters_0604.pkl\n",
      " 0611: 117 clusters saved to ../../data/added_model_clusters_0611.pkl\n",
      " 0618: 221 clusters saved to ../../data/added_model_clusters_0618.pkl\n",
      " 0625: 48 clusters saved to ../../data/added_model_clusters_0625.pkl\n",
      " 0702: 153 clusters saved to ../../data/added_model_clusters_0702.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "def bfs(model, graph):\n",
    "    visited = set()\n",
    "    queue = deque([model])\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            for neighbor in graph.get(node, []):\n",
    "                if neighbor not in visited:\n",
    "                    queue.append(neighbor)\n",
    "    return visited\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    csv_path = os.path.join(base_dir, f\"added_models/added_model_{date_str}.csv\")\n",
    "    output_path = os.path.join(base_dir, f\"added_model_clusters/added_model_clusters_{date_str}.pkl\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Missing file: {csv_path}\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path).fillna(\"\")\n",
    "\n",
    "        forward_graph = defaultdict(set)\n",
    "        reverse_graph = defaultdict(set)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            model = row[\"Model ID\"]\n",
    "            base_model = row[\"Base Model\"]\n",
    "            model_type = str(row[\"Type\"]).lower()\n",
    "\n",
    "            if base_model and base_model != \"N/A\":\n",
    "                base_models = [m.strip() for m in base_model.split(\",\")] if model_type == \"merge\" else [base_model.strip()]\n",
    "                for base in base_models:\n",
    "                    if base:\n",
    "                        forward_graph[base].add(model)\n",
    "                        reverse_graph[model].add(base)\n",
    "            else:\n",
    "                forward_graph[model] = forward_graph.get(model, set())\n",
    "\n",
    "        all_models = set(df[\"Model ID\"]) | set(m for v in forward_graph.values() for m in v)\n",
    "        root_models = [m for m in all_models if m not in reverse_graph]\n",
    "\n",
    "        cluster_dict = {}\n",
    "        for root in root_models:\n",
    "            cluster = bfs(root, forward_graph)\n",
    "            if len(cluster) > 1:\n",
    "                cluster_dict[root] = cluster\n",
    "\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump(cluster_dict, f)\n",
    "\n",
    "        print(f\" {date_str}: {len(cluster_dict)} clusters saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {e}\")\n",
    "\n",
    "    current += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The size of added model clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0319: 236 clusters, avg size = 3.10\n",
      "0326: 239 clusters, avg size = 2.64\n",
      "0402: 242 clusters, avg size = 2.53\n",
      "0409: 217 clusters, avg size = 2.79\n",
      "0416: 195 clusters, avg size = 2.89\n",
      "0423: 187 clusters, avg size = 2.80\n",
      "0430: 178 clusters, avg size = 5.74\n",
      "0507: 159 clusters, avg size = 2.87\n",
      "0514: 164 clusters, avg size = 2.62\n",
      "0521: 197 clusters, avg size = 2.65\n",
      "0528: 117 clusters, avg size = 2.65\n",
      "0604: 151 clusters, avg size = 4.08\n",
      "0611: 117 clusters, avg size = 2.63\n",
      "0618: 221 clusters, avg size = 2.57\n",
      "0625: 48 clusters, avg size = 2.67\n",
      "0702: 153 clusters, avg size = 3.04\n",
      "\n",
      "\n",
      "Overall average size : 3.01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "weekly_averages = []\n",
    "all_cluster_sizes = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    cluster_path = os.path.join(base_dir, f\"added_model_clusters/added_model_clusters_{date_str}.pkl\")\n",
    "\n",
    "    if not os.path.exists(cluster_path):\n",
    "        print(f\"Missing file for {date_str}\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(cluster_path, \"rb\") as f:\n",
    "            cluster_dict = pickle.load(f)\n",
    "\n",
    "        cluster_sizes = [len(models) for models in cluster_dict.values()]\n",
    "        if cluster_sizes:\n",
    "            avg_size = sum(cluster_sizes) / len(cluster_sizes)\n",
    "            weekly_averages.append(avg_size)\n",
    "            all_cluster_sizes.extend(cluster_sizes)\n",
    "            print(f\"{date_str}: {len(cluster_sizes)} clusters, avg size = {avg_size:.2f}\")\n",
    "        else:\n",
    "            print(f\"{date_str}: No clusters found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {e}\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if weekly_averages:\n",
    "    overall_avg = sum(all_cluster_sizes) / len(all_cluster_sizes) if all_cluster_sizes else 0\n",
    "    print(\"\\n\")\n",
    "    print(f\"Overall average size : {overall_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affected models by base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0319: 225 models affected,  deleted base models: 127\n",
      "\n",
      "0326: 222 models affected,  deleted base models: 134\n",
      "\n",
      "0402: 183 models affected,  deleted base models: 100\n",
      "\n",
      "0409: 278 models affected,  deleted base models: 181\n",
      "\n",
      "0416: 1318 models affected,  deleted base models: 131\n",
      "\n",
      "0423: 217 models affected,  deleted base models: 112\n",
      "\n",
      "0430: 187 models affected,  deleted base models: 105\n",
      "\n",
      "0507: 517 models affected,  deleted base models: 292\n",
      "\n",
      "0514: 100 models affected,  deleted base models: 59\n",
      "\n",
      "0521: 335 models affected,  deleted base models: 213\n",
      "\n",
      "0528: 272 models affected,  deleted base models: 133\n",
      "\n",
      "0604: 267 models affected,  deleted base models: 141\n",
      "\n",
      "0611: 193 models affected,  deleted base models: 73\n",
      "\n",
      "0618: 204 models affected,  deleted base models: 130\n",
      "\n",
      "0625: 135 models affected,  deleted base models: 28\n",
      "\n",
      "0702: 131 models affected,  deleted base models: 75\n",
      "\n",
      "Average affected models weekly:299.0 \n",
      "Average deleted base models weekly:127.12 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "batch_files = {\n",
    "    os.path.basename(f).replace(\"batch_all_\", \"\").replace(\".csv\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_relation/batch_all_*.csv\"))\n",
    "}\n",
    "\n",
    "weekly_downstream_results = {}\n",
    "\n",
    "for del_path in deleted_files:\n",
    "    del_date = os.path.basename(del_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        del_dt = datetime.strptime(del_date, \"%m%d\")\n",
    "        batch_dt = del_dt - timedelta(days=7)\n",
    "        batch_date = batch_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse date failed: {del_date},skipping.Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    batch_path = batch_files.get(batch_date)\n",
    "    if not batch_path:\n",
    "        print(f\"File cannot be found: batch_all_{batch_date}.csv.Skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(del_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    downstream_models = set()\n",
    "    affected_base_models = set()\n",
    "    with open(batch_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            base_model = row[\"Base Model\"].strip()\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if base_model in deleted_models:\n",
    "                downstream_models.add(model_id)\n",
    "                affected_base_models.add(base_model)\n",
    "\n",
    "    weekly_downstream_results[del_date] = {\n",
    "        \"downstream\": downstream_models,\n",
    "        \"base_models\": affected_base_models\n",
    "    }\n",
    "\n",
    "for del_date, data in weekly_downstream_results.items():\n",
    "    downstream_models = data[\"downstream\"]\n",
    "    affected_base_models = data[\"base_models\"]\n",
    "    print(f\"\\n{del_date}: {len(downstream_models)} models affected,  deleted base models: {len(affected_base_models)}\")\n",
    "\n",
    "total_downstream = sum(len(data[\"downstream\"]) for data in weekly_downstream_results.values())\n",
    "total_affected_base_models = sum(len(data[\"base_models\"]) for data in weekly_downstream_results.values())\n",
    "weeks = len(weekly_downstream_results)\n",
    "avg_downstream = total_downstream / weeks if weeks else 0\n",
    "avg_affected_base_models = total_affected_base_models / weeks if weeks else 0\n",
    "\n",
    "print(f\"\\nAverage affected models weekly:{round(avg_downstream, 2)} \")\n",
    "print(f\"Average deleted base models weekly:{round(avg_affected_base_models, 2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disrupted chains in 17 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0319:\n",
      "affected models: 337\n",
      "disrupted chains:1965\n",
      "\n",
      " 0326:\n",
      "affected models: 313\n",
      "disrupted chains:698\n",
      "\n",
      " 0402:\n",
      "affected models: 1107\n",
      "disrupted chains:2667\n",
      "\n",
      " 0409:\n",
      "affected models: 331\n",
      "disrupted chains:2650\n",
      "\n",
      " 0416:\n",
      "affected models: 1921\n",
      "disrupted chains:2564\n",
      "\n",
      " 0423:\n",
      "affected models: 569\n",
      "disrupted chains:1503\n",
      "\n",
      " 0430:\n",
      "affected models: 452\n",
      "disrupted chains:1105\n",
      "\n",
      " 0507:\n",
      "affected models: 1155\n",
      "disrupted chains:5850\n",
      "\n",
      " 0514:\n",
      "affected models: 151\n",
      "disrupted chains:491\n",
      "\n",
      " 0521:\n",
      "affected models: 479\n",
      "disrupted chains:802\n",
      "\n",
      " 0528:\n",
      "affected models: 302\n",
      "disrupted chains:326\n",
      "\n",
      " 0604:\n",
      "affected models: 279\n",
      "disrupted chains:300\n",
      "\n",
      " 0611:\n",
      "affected models: 239\n",
      "disrupted chains:372\n",
      "\n",
      " 0618:\n",
      "affected models: 240\n",
      "disrupted chains:491\n",
      "\n",
      " 0625:\n",
      "affected models: 212\n",
      "disrupted chains:313\n",
      "\n",
      " 0702:\n",
      "affected models: 361\n",
      "disrupted chains:952\n",
      "\n",
      "Average affected downstream models:528.0 \n",
      "Average disrupted chains: 1440.56 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "chain_files = {\n",
    "    os.path.basename(f).replace(\"model_chains_\", \"\").replace(\".txt\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_chains/model_chains_*.txt\"))\n",
    "}\n",
    "\n",
    "weekly_chain_impact = {}\n",
    "\n",
    "for del_path in deleted_files:\n",
    "    del_date = os.path.basename(del_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        del_dt = datetime.strptime(del_date, \"%m%d\")\n",
    "        chain_dt = del_dt - timedelta(days=7)\n",
    "        chain_date = chain_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse date failed: {del_date},skipping.Error : {e}\")\n",
    "        continue\n",
    "\n",
    "    chain_path = chain_files.get(chain_date)\n",
    "    if not chain_path:\n",
    "        print(f\"File cannot be found: model_chains_{chain_date}.txt,Skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(del_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    affected_models = set()\n",
    "    affected_chain_count = 0\n",
    "    with open(chain_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"->\" not in line:\n",
    "                continue\n",
    "            models = [m.strip() for m in line.strip().split(\"->\")]\n",
    "\n",
    "            chain_impacted = False\n",
    "\n",
    "            for i, m in enumerate(models):\n",
    "                if m in deleted_models and i < len(models) - 1:\n",
    "                    affected_models.update(models[i+1:])\n",
    "                    chain_impacted = True\n",
    "\n",
    "            if chain_impacted:\n",
    "                affected_chain_count += 1\n",
    "\n",
    "    weekly_chain_impact[del_date] = {\n",
    "        \"affected_models\": affected_models,\n",
    "        \"affected_chains\": affected_chain_count\n",
    "    }\n",
    "\n",
    "for del_date, data in weekly_chain_impact.items():\n",
    "    print(f\"\\n {del_date}:\")\n",
    "    print(f\"affected models: {len(data['affected_models'])}\")\n",
    "    print(f\"disrupted chains:{data['affected_chains']}\")\n",
    "\n",
    "total_affected_models = sum(len(data[\"affected_models\"]) for data in weekly_chain_impact.values())\n",
    "total_affected_chains = sum(data[\"affected_chains\"] for data in weekly_chain_impact.values())\n",
    "weeks = len(weekly_chain_impact)\n",
    "\n",
    "avg_affected_models = total_affected_models / weeks if weeks else 0\n",
    "avg_affected_chains = total_affected_chains / weeks if weeks else 0\n",
    "\n",
    "print(f\"\\nAverage affected downstream models:{round(avg_affected_models, 2)} \")\n",
    "print(f\"Average disrupted chains: {round(avg_affected_chains, 2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disrupted clusters in 17 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average size of disrupted models weekly\n",
      "03190312: 81,size:762.16\n",
      "03260319: 73,size:931.77\n",
      "04020326: 109,size:1097.19\n",
      "04090402: 244,size:742.0\n",
      "04160409: 64,size:1001.31\n",
      "04230416: 77,size:1126.69\n",
      "04300423: 35,size:1967.77\n",
      "05070430: 79,size:903.81\n",
      "05140507: 86,size:1159.49\n",
      "05210514: 85,size:1771.92\n",
      "05280521: 38,size:2008.53\n",
      "06040528: 51,size:1793.75\n",
      "06110604: 60,size:1182.95\n",
      "06180611: 56,size:1892.62\n",
      "06250618: 25,size:1506.64\n",
      "07020625: 81,size:1143.99\n",
      "\n",
      "The average size of all disrupted models1163.13\n",
      "The average num of disrupted models weekly: 77.75 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "cluster_files = {\n",
    "    os.path.basename(f).replace(\"model_clusters_\", \"\").replace(\".pkl\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_clusters/model_clusters_*.pkl\"))\n",
    "}\n",
    "batch_files = {\n",
    "    os.path.basename(f).replace(\"batch_all_\", \"\").replace(\".csv\", \"\"): f\n",
    "    for f in glob.glob(os.path.join(data_dir, \"model_relation/batch_all_*.csv\"))\n",
    "}\n",
    "\n",
    "weekly_results = []\n",
    "all_broken_cluster_sizes = []\n",
    "all_broken_cluster_counts = []\n",
    "\n",
    "for del_path in deleted_files:\n",
    "    del_date = os.path.basename(del_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        del_dt = datetime.strptime(del_date, \"%m%d\")\n",
    "        cluster_dt = del_dt - timedelta(days=7)\n",
    "        cluster_date = cluster_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse date failed: {del_date},skipping.Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    cluster_path = cluster_files.get(cluster_date)\n",
    "    batch_path = batch_files.get(cluster_date)\n",
    "\n",
    "    if not cluster_path:\n",
    "        print(f\"Files cannot be found: model_clusters_{cluster_date}.pkl.Skipping.\")\n",
    "        continue\n",
    "    if not batch_path:\n",
    "        print(f\"Files cannot be found: batch_all_{cluster_date}.csv.Skipping.\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(del_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    deleted_with_downstream = set()\n",
    "    with open(batch_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            base_model = row[\"Base Model\"].strip()\n",
    "            if base_model in deleted_models:\n",
    "                deleted_with_downstream.add(base_model)\n",
    "\n",
    "    with open(cluster_path, \"rb\") as f:\n",
    "        cluster_data = pickle.load(f)\n",
    "\n",
    "    broken_cluster_sizes = []\n",
    "    for cluster_id, models in cluster_data.items():\n",
    "        if any(m in deleted_with_downstream for m in models):\n",
    "            broken_cluster_sizes.append(len(models))\n",
    "            all_broken_cluster_sizes.append(len(models))\n",
    "\n",
    "    broken_count = len(broken_cluster_sizes)\n",
    "    avg_size = round(sum(broken_cluster_sizes) / broken_count, 2) if broken_count > 0 else 0\n",
    "    weekly_results.append((del_date, cluster_date, broken_count, avg_size))\n",
    "    all_broken_cluster_counts.append(broken_count)\n",
    "\n",
    "print(\"The average size of disrupted models weekly\")\n",
    "for del_date, cluster_date, count, avg_size in weekly_results:\n",
    "    print(f\"{del_date}{cluster_date}: {count},size:{avg_size}\")\n",
    "\n",
    "total_clusters = len(all_broken_cluster_sizes)\n",
    "total_avg_size = round(sum(all_broken_cluster_sizes) / total_clusters, 2) if total_clusters > 0 else 0\n",
    "\n",
    "weeks = len(all_broken_cluster_counts)\n",
    "avg_broken_clusters_per_week = round(sum(all_broken_cluster_counts) / weeks, 2) if weeks else 0\n",
    "\n",
    "print(f\"\\nThe average size of all disrupted models{total_avg_size}\")\n",
    "print(f\"The average num of disrupted models weekly: {avg_broken_clusters_per_week} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top chains and clusters analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. select top-10 chains in 17 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top-10 chains to: ../../data/top10_chains_0312.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0319.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0326.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0402.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0409.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0416.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0423.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0430.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0507.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0514.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0521.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0528.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0604.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0611.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0618.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0625.txt\n",
      "Saved top-10 chains to: ../../data/top10_chains_0702.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-12\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    input_file = os.path.join(base_dir, f\"model_depth_{date_str}.txt\")\n",
    "    output_file = os.path.join(base_dir, f\"top10_chains_{date_str}.txt\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\" File not found: {input_file}\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        chains = []\n",
    "        in_chain_section = False\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.strip() == \"Longest Paths:\":\n",
    "                in_chain_section = True\n",
    "                continue\n",
    "            if in_chain_section:\n",
    "                if line.strip().startswith(\"Depth\") or line.strip() == \"\" or not line.startswith(\"  \"):\n",
    "                    in_chain_section = False\n",
    "                    continue\n",
    "                chain = line.strip()\n",
    "                chain_len = chain.count(\"->\")\n",
    "                chains.append((chain_len, chain))\n",
    "\n",
    "        top10_chains = sorted(chains, key=lambda x: x[0], reverse=True)[:10]\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "            for _, chain in top10_chains:\n",
    "                out_f.write(chain + \"\\n\")\n",
    "\n",
    "        print(f\"Saved top-10 chains to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {date_str}: {e}\")\n",
    "\n",
    "    current += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. select top-10 clusters in 17 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved Top-10 models for 0312 to ../../data/top10_model_dis_0312.txt\n",
      " Saved Top-10 models for 0319 to ../../data/top10_model_dis_0319.txt\n",
      " Saved Top-10 models for 0326 to ../../data/top10_model_dis_0326.txt\n",
      " Saved Top-10 models for 0402 to ../../data/top10_model_dis_0402.txt\n",
      " Saved Top-10 models for 0409 to ../../data/top10_model_dis_0409.txt\n",
      " Saved Top-10 models for 0416 to ../../data/top10_model_dis_0416.txt\n",
      " Saved Top-10 models for 0423 to ../../data/top10_model_dis_0423.txt\n",
      " Saved Top-10 models for 0430 to ../../data/top10_model_dis_0430.txt\n",
      " Saved Top-10 models for 0507 to ../../data/top10_model_dis_0507.txt\n",
      " Saved Top-10 models for 0514 to ../../data/top10_model_dis_0514.txt\n",
      " Saved Top-10 models for 0521 to ../../data/top10_model_dis_0521.txt\n",
      " Saved Top-10 models for 0528 to ../../data/top10_model_dis_0528.txt\n",
      " Saved Top-10 models for 0604 to ../../data/top10_model_dis_0604.txt\n",
      " Saved Top-10 models for 0611 to ../../data/top10_model_dis_0611.txt\n",
      " Saved Top-10 models for 0618 to ../../data/top10_model_dis_0618.txt\n",
      " Saved Top-10 models for 0625 to ../../data/top10_model_dis_0625.txt\n",
      " Saved Top-10 models for 0702 to ../../data/top10_model_dis_0702.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(\"2025-03-12\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    input_file = os.path.join(base_dir, f\"model_cluster_size_{date_str}.csv\")\n",
    "    output_file = os.path.join(base_dir, f\"top10_cluster_size_{date_str}.txt\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        top10_models = df.sort_values(\"DIS\", ascending=False).head(10)[\"Model Name\"].tolist()\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for model in top10_models:\n",
    "                f.write(model + \"\\n\")\n",
    "\n",
    "        print(f\" Saved Top-10 models for {date_str} to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {input_file}: {e}\")\n",
    "\n",
    "    current += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. added model in top-10 chains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0319: 0 models in top-10 chains\n",
      "0326: 0 models in top-10 chains\n",
      "0402: 0 models in top-10 chains\n",
      "0409: 0 models in top-10 chains\n",
      "0416: 0 models in top-10 chains\n",
      "0423: 0 models in top-10 chains\n",
      "0430: 0 models in top-10 chains\n",
      "0507: 0 models in top-10 chains\n",
      "0514: 0 models in top-10 chains\n",
      "0521: 0 models in top-10 chains\n",
      "0528: 0 models in top-10 chains\n",
      "0604: 0 models in top-10 chains\n",
      "0611: 0 models in top-10 chains\n",
      "0618: 0 models in top-10 chains\n",
      "0625: 0 models in top-10 chains\n",
      "0702: 0 models in top-10 chains\n",
      "\n",
      " Average:  0.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_dir = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "matched_counts = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    chains_file = os.path.join(base_dir, f\"top10_chains_{date_str}.txt\")\n",
    "    added_file = os.path.join(base_dir, f\"added_models/added_model_{date_str}.csv\")\n",
    "\n",
    "    if not (os.path.exists(chains_file) and os.path.exists(added_file)):\n",
    "        print(f\" Missing file for {date_str}, skipped.\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "\n",
    "    chain_models = set()\n",
    "    with open(chains_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = [m.strip() for m in line.strip().split(\"->\")]\n",
    "            chain_models.update(parts)\n",
    "\n",
    "    df_added = pd.read_csv(added_file)\n",
    "    added_models = set(df_added[\"Model ID\"].dropna())\n",
    "\n",
    "    matched_models = added_models & chain_models\n",
    "    count = len(matched_models)\n",
    "    matched_counts.append(count)\n",
    "\n",
    "    print(f\"{date_str}: {count} models in top-10 chains\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if matched_counts:\n",
    "    avg_count = sum(matched_counts) / len(matched_counts)\n",
    "    print(\"\\n Average: \", f\"{avg_count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. added models in top-10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0319: 1554 models in top-10 clusters\n",
      "0326: 1863 models in top-10 clusters\n",
      "0402: 1986 models in top-10 clusters\n",
      "0409: 1967 models in top-10 clusters\n",
      "0416: 1265 models in top-10 clusters\n",
      "0423: 1082 models in top-10 clusters\n",
      "0430: 1742 models in top-10 clusters\n",
      "0507: 1512 models in top-10 clusters\n",
      "0514: 1178 models in top-10 clusters\n",
      "0521: 1322 models in top-10 clusters\n",
      "0528: 997 models in top-10 clusters\n",
      "0604: 1473 models in top-10 clusters\n",
      "0611: 1095 models in top-10 clusters\n",
      "0618: 1730 models in top-10 clusters\n",
      "0625: 358 models in top-10 clusters\n",
      "0702: 1033 models in top-10 clusters\n",
      "\n",
      " Average:  1384.81\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "base_path = \"../../data\"\n",
    "start_date = datetime.strptime(\"2025-03-19\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-07-02\", \"%Y-%m-%d\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "matched_counts = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    date_str = current.strftime(\"%m%d\")\n",
    "    cluster_file = f\"{base_path}/model_clusters/model_clusters_{date_str}.pkl\"\n",
    "    added_file = f\"{base_path}/added_models/added_model_{date_str}.csv\"\n",
    "\n",
    "    if not os.path.exists(cluster_file) or not os.path.exists(added_file):\n",
    "        print(f\"Missing file for {date_str}, skipped.\")\n",
    "        current += delta\n",
    "        continue\n",
    "\n",
    "    with open(cluster_file, \"rb\") as f:\n",
    "        cluster_dict = pickle.load(f)\n",
    "\n",
    "    top10_clusters = sorted(cluster_dict.values(), key=len, reverse=True)[:10]\n",
    "    top10_models = set()\n",
    "    for cluster in top10_clusters:\n",
    "        top10_models.update(cluster)\n",
    "\n",
    "    df_added = pd.read_csv(added_file)\n",
    "    added_models = set(df_added[\"Model ID\"].dropna())\n",
    "\n",
    "    matched_models = added_models & top10_models\n",
    "    matched_counts.append(len(matched_models))\n",
    "\n",
    "    print(f\"{date_str}: {len(matched_models)} models in top-10 clusters\")\n",
    "\n",
    "    current += delta\n",
    "\n",
    "if matched_counts:\n",
    "    avg_count = sum(matched_counts) / len(matched_counts)\n",
    "    print(\"\\n Average: \", f\"{avg_count:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. deleted models in top-10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weekly Top-10 Cluster Impact Summary\n",
      "[0319] ← Clusters(0312): 8 affected clusters, 135 of 4289 deleted models in top10 (3.1476%)\n",
      "[0326] ← Clusters(0319): 8 affected clusters, 236 of 4800 deleted models in top10 (4.9167%)\n",
      "[0402] ← Clusters(0326): 8 affected clusters, 226 of 14243 deleted models in top10 (1.5867%)\n",
      "[0409] ← Clusters(0402): 9 affected clusters, 203 of 3157 deleted models in top10 (6.4302%)\n",
      "[0416] ← Clusters(0409): 9 affected clusters, 111 of 4251 deleted models in top10 (2.6112%)\n",
      "[0423] ← Clusters(0416): 8 affected clusters, 137 of 4075 deleted models in top10 (3.362%)\n",
      "[0430] ← Clusters(0423): 8 affected clusters, 101 of 4224 deleted models in top10 (2.3911%)\n",
      "[0507] ← Clusters(0430): 10 affected clusters, 1197 of 19649 deleted models in top10 (6.0919%)\n",
      "[0514] ← Clusters(0507): 6 affected clusters, 102 of 4725 deleted models in top10 (2.1587%)\n",
      "[0521] ← Clusters(0514): 10 affected clusters, 206 of 4553 deleted models in top10 (4.5245%)\n",
      "[0528] ← Clusters(0521): 8 affected clusters, 133 of 6990 deleted models in top10 (1.9027%)\n",
      "[0604] ← Clusters(0528): 8 affected clusters, 243 of 4842 deleted models in top10 (5.0186%)\n",
      "[0611] ← Clusters(0604): 8 affected clusters, 303 of 8838 deleted models in top10 (3.4284%)\n",
      "[0618] ← Clusters(0611): 7 affected clusters, 188 of 5048 deleted models in top10 (3.7242%)\n",
      "[0625] ← Clusters(0618): 7 affected clusters, 90 of 2516 deleted models in top10 (3.5771%)\n",
      "[0702] ← Clusters(0625): 7 affected clusters, 204 of 2842 deleted models in top10 (7.178%)\n",
      "\n",
      "Overall Average:\n",
      "- Average affected Top-10 clusters: 8.06\n",
      "- Average deleted models in Top-10 clusters weekly: 238.44\n",
      "- Average percentage: 3.88%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "data_dir = \"../../data\"\n",
    "deleted_files = sorted(glob.glob(os.path.join(data_dir, \"deleted_models/deleted_model_*.csv\")))\n",
    "\n",
    "weekly_results = []\n",
    "\n",
    "for deleted_path in deleted_files:\n",
    "    deleted_date = os.path.basename(deleted_path).replace(\"deleted_model_\", \"\").replace(\".csv\", \"\")\n",
    "    try:\n",
    "        deleted_dt = datetime.strptime(deleted_date, \"%m%d\")\n",
    "        cluster_dt = deleted_dt - timedelta(days=7)\n",
    "        cluster_date = cluster_dt.strftime(\"%m%d\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parse date failed: {deleted_date} -> {e}\")\n",
    "        continue\n",
    "\n",
    "    cluster_path = os.path.join(data_dir, f\"model_clusters/model_clusters_{cluster_date}.pkl\")\n",
    "    if not os.path.exists(cluster_path):\n",
    "        print(f\"File cannot be found: {cluster_path}\")\n",
    "        continue\n",
    "\n",
    "    deleted_models = set()\n",
    "    with open(deleted_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            model_id = row[\"Model ID\"].strip()\n",
    "            if model_id:\n",
    "                deleted_models.add(model_id)\n",
    "\n",
    "    with open(cluster_path, \"rb\") as f:\n",
    "        cluster_dict = pickle.load(f) \n",
    "\n",
    "    clusters_sorted = sorted(cluster_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    top10_clusters = clusters_sorted[:10]\n",
    "\n",
    "    affected_cluster_count = 0\n",
    "    deleted_models_in_top10 = set()\n",
    "\n",
    "    for cluster_id, models in top10_clusters:\n",
    "        intersected = deleted_models.intersection(models)\n",
    "        if intersected:\n",
    "            affected_cluster_count += 1\n",
    "            deleted_models_in_top10.update(intersected)\n",
    "\n",
    "    weekly_results.append({\n",
    "        \"deleted_date\": deleted_date,\n",
    "        \"cluster_date\": cluster_date,\n",
    "        \"affected_top10_clusters\": affected_cluster_count,\n",
    "        \"deleted_model_count\": len(deleted_models),\n",
    "        \"deleted_models_in_top10\": len(deleted_models_in_top10),\n",
    "        \"percent\": round(100 * len(deleted_models_in_top10) / len(deleted_models), 4) if deleted_models else 0.0\n",
    "    })\n",
    "\n",
    "print(\"\\n Weekly Top-10 Cluster Impact Summary\")\n",
    "for r in weekly_results:\n",
    "    print(f\"[{r['deleted_date']}] ← Clusters({r['cluster_date']}): \"\n",
    "          f\"{r['affected_top10_clusters']} affected clusters, \"\n",
    "          f\"{r['deleted_models_in_top10']} of {r['deleted_model_count']} deleted models in top10 \"\n",
    "          f\"({r['percent']}%)\")\n",
    "\n",
    "if weekly_results:\n",
    "    avg_clusters = sum(r[\"affected_top10_clusters\"] for r in weekly_results) / len(weekly_results)\n",
    "    avg_models = sum(r[\"deleted_models_in_top10\"] for r in weekly_results) / len(weekly_results)\n",
    "    avg_percent = sum(r[\"percent\"] for r in weekly_results) / len(weekly_results)\n",
    "    print(\"\\nOverall Average:\")\n",
    "    print(f\"- Average affected Top-10 clusters: {avg_clusters:.2f}\")\n",
    "    print(f\"- Average deleted models in Top-10 clusters weekly: {avg_models:.2f}\")\n",
    "    print(f\"- Average percentage: {avg_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of updated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Updated Models by Structural Type ===\n",
      "Derived models : 37,685 models\n",
      "Base models: 13,666 models\n",
      "Isolated models: 99,168 models\n",
      "Relational models: 46,277 models\n",
      "All updated models: 145,445 models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "degree_csv = \"../../data/model_degree_0702.csv\"\n",
    "implicit_file = \"../../data/rq2_update_models.txt\"\n",
    "\n",
    "degree_df = pd.read_csv(degree_csv)\n",
    "\n",
    "with open(implicit_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    implicit_models = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "implicit_df = degree_df[degree_df[\"Model ID\"].isin(implicit_models)]\n",
    "\n",
    "derived_models = implicit_df[implicit_df[\"In-degree\"] > 0]\n",
    "base_models = implicit_df[implicit_df[\"Out-degree\"] > 0]\n",
    "isolated_models = implicit_df[(implicit_df[\"In-degree\"] == 0) & (implicit_df[\"Out-degree\"] == 0)]\n",
    "relational_models = implicit_df[(implicit_df[\"In-degree\"] > 0) | (implicit_df[\"Out-degree\"] > 0)]\n",
    "total_models = implicit_df\n",
    "\n",
    "def print_model_stats(name, df):\n",
    "    print(f\"{name}: {len(df):,} models\")\n",
    "\n",
    "print(\"=== Updated Models by Structural Type ===\")\n",
    "print_model_stats(\"Derived models \", derived_models)\n",
    "print_model_stats(\"Base models\", base_models)\n",
    "print_model_stats(\"Isolated models\", isolated_models)\n",
    "print_model_stats(\"Relational models\", relational_models)\n",
    "print_model_stats(\"All updated models\", total_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation distribution of updated models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Updated Models by Dependency Type ===\n",
      "Total updated models with dependency type: 145445\n",
      "n/a: 107760 (74.09%)\n",
      "finetune: 30872 (21.23%)\n",
      "quantized: 5704 (3.92%)\n",
      "merge: 1109 (0.76%)\n",
      "\n",
      "Dependency tuples: 37685 (25.91%)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "implicit_update_file = \"../../data/rq2_update_models.txt\"\n",
    "batch_all_file = \"../../data/model_relation/batch_all_0702.csv\"\n",
    "\n",
    "with open(implicit_update_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    implicit_models = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "type_counter = Counter()\n",
    "total = 0\n",
    "\n",
    "with open(batch_all_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        model_id = row.get(\"Model ID\")\n",
    "        if model_id not in implicit_models:\n",
    "            continue \n",
    "\n",
    "        type_raw = row.get(\"Type\", \"\").lower()\n",
    "        type_normalized = \"finetune\" if type_raw == \"adapter\" else type_raw\n",
    "        type_counter[type_normalized] += 1\n",
    "        total += 1\n",
    "\n",
    "print(\"=== Updated Models by Dependency Type ===\")\n",
    "print(f\"Total updated models with dependency type: {total}\")\n",
    "for model_type, count in type_counter.items():\n",
    "    pct = (count / total) * 100\n",
    "    print(f\"{model_type}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "key_types = [\"finetune\", \"quantized\", \"merge\"]\n",
    "key_total = sum(type_counter.get(k, 0) for k in key_types)\n",
    "key_pct = (key_total / total) * 100\n",
    "print(f\"\\nDependency tuples: {key_total} ({key_pct:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "czq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
